# UltraScript Goroutine System V2: Go-Style Implementation

## Overview

This document outlines the complete design for implementing a Go-style goroutine system with proper stack switching, thread pools, and high-performance async handling. This is a comprehensive rewrite that will provide true lightweight concurrency with minimal overhead.

## Architecture Decision: Go-Style FFI Thread Binding System

Based on analysis of both web server patterns (100k+ concurrent requests) and ML workloads (thousands of FFI calls), we implement a **Go-style automatic FFI thread binding system** that provides both massive concurrency AND zero FFI overhead transparently.

### Core Innovation: Dynamic Thread Binding

**How it Works**: All goroutines start lightweight on event-driven thread pools. When a goroutine makes its first FFI call, it gets **automatically bound** to a dedicated OS thread where all subsequent FFI calls have zero overhead.

### Web Server Goroutines (Stay Lightweight)
```ultraScript
// Handles 100k+ concurrent requests - never makes FFI calls, stays on thread pool
server.on('request', go async (req, res) => {
    const user = await database.getUser(req.userId);  // I/O operation, stays lightweight
    const data = await api.fetchUserData(user.id);    // HTTP call, stays lightweight
    res.json(data);
});
```

### ML/FFI Goroutines (Auto-Bind to Threads)
```ultraScript
// Automatically gets bound to OS thread on first FFI call
go async function trainModel() {
    const tensor = torch.zeros([1000, 1000]); // FIRST FFI CALL -> goroutine bound to OS thread
    const result = model.forward(tensor);     // ZERO overhead - same thread
    const loss = criterion(result, target);  // ZERO overhead - same thread
    return loss.backward();                   // ZERO overhead - same thread
}
```

### Benefits of This Approach
✅ **Unlimited web server concurrency**: 100k+ I/O-bound goroutines stay lightweight  
✅ **Zero FFI overhead**: ML goroutines get dedicated threads automatically  
✅ **Transparent operation**: Runtime handles optimization, developers don't choose modes  
✅ **Go-proven architecture**: Based on Go's successful cgo implementation  
✅ **Hybrid scaling**: 100k I/O goroutines + 1k FFI goroutines simultaneously  
✅ **Best of both worlds**: No trade-offs, optimal for all use cases

## Architecture Components

### 1. Thread Pool Management

**Core Design:**
- Fixed number of OS threads (typically `std::thread::hardware_concurrency()`)
- **Event-driven architecture** - threads sleep when idle, wake only when work arrives
- Global priority queues with intelligent thread affinity for cache locality
- Zero CPU usage when no work available (similar to Go's scheduler)

**Thread Worker Structure:**
```cpp
class ThreadWorker {
    int thread_id_;
    std::atomic<bool> is_idle_{true};
    std::atomic<bool> should_exit_{false};
    
    // Work assignment and wake-up mechanism
    std::shared_ptr<Goroutine> assigned_work_{nullptr};
    std::condition_variable work_signal_;
    std::mutex work_mutex_;
    
    // Stack depth tracking for trampoline
    int stack_depth_{0};
    static constexpr int MAX_STACK_DEPTH = 100;
    
public:
    void main_loop();                    // Event-driven main loop
    bool try_assign_work(std::shared_ptr<Goroutine> goroutine);
    bool try_assign_queued_work();      // NEW: Wake thread to check queues (race condition fix)
    void wait_for_work();               // Blocks until work arrives
    void wake_for_work();               // Wakes thread for assigned work
};
```

### FFI Thread Pool for Zero-Overhead FFI Calls

**FFI Thread Management** - dedicated OS threads for FFI-bound goroutines:
```cpp
class FFIThread {
private:
    std::thread native_thread_;
    std::atomic<bool> is_bound_{false};
    std::shared_ptr<Goroutine> bound_goroutine_{nullptr};
    std::condition_variable work_signal_;
    std::mutex work_mutex_;
    
public:
    void bind_goroutine(std::shared_ptr<Goroutine> goroutine);
    void execute_with_native_stack();  // Runs goroutine on native OS thread stack
    bool is_available() const { return !is_bound_; }
    void release_binding();
};

class FFIThreadPool {
private:
    std::vector<std::unique_ptr<FFIThread>> ffi_threads_;
    std::atomic<int> available_count_{1000};  // Pre-create 1000 FFI threads
    std::mutex allocation_mutex_;
    
public:
    FFIThread* acquire_thread_for_binding();
    void release_thread(FFIThread* thread);
    void initialize_pool(int thread_count = 1000);
};
```

### Automatic FFI Detection and Binding

**JIT-Compiled FFI Binding Logic:**
```cpp
// Generated by JIT compiler for FFI calls
void execute_ffi_call(Goroutine* current_goroutine, void* ffi_function, void* args) {
    if (current_goroutine->is_ffi_bound()) {
        // ZERO OVERHEAD PATH: Direct call on dedicated thread
        ((FFIFunction)ffi_function)(args);
    } else {
        // FIRST FFI CALL: Bind goroutine to dedicated OS thread
        FFIThread* ffi_thread = global_scheduler.acquire_ffi_thread();
        ffi_thread->bind_goroutine(current_goroutine);
        current_goroutine->set_ffi_bound(true);
        
        // Migrate stack and execute FFI call on native thread
        ffi_thread->execute_with_native_stack();
        ((FFIFunction)ffi_function)(args);
    }
}
```

### 2. Goroutine Data Structure

**Complete Goroutine State:**
```cpp
struct GoroutineContext {
    // CPU Register State (x86_64)
    uint64_t rax, rbx, rcx, rdx, rsi, rdi, rbp;
    uint64_t r8, r9, r10, r11, r12, r13, r14, r15;
    uint64_t rsp;        // Stack pointer
    uint64_t rip;        // Instruction pointer
    uint64_t rflags;     // CPU flags
    
    // XMM/AVX registers for floating point
    alignas(16) uint8_t xmm_state[512]; // FXSAVE area
    
    // Stack management
    void* stack_base;
    void* stack_top;
    size_t stack_size;
    void* guard_page;
};

class Goroutine {
private:
    int64_t id_;
    std::atomic<GoroutineState> state_{GoroutineState::CREATED};
    std::atomic<bool> is_running_{false};
    std::atomic<int> preferred_thread_id_{-1}; // -1 = no preference, for cache locality
    
    // FFI Thread Binding (NEW)
    std::atomic<bool> is_ffi_bound_{false};     // Has this goroutine been bound to an OS thread?
    FFIThread* bound_ffi_thread_{nullptr};     // Which FFI thread is this bound to?
    std::atomic<int> ffi_call_count_{0};       // Track number of FFI calls for optimization
    
    // Execution context
    GoroutineContext context_;
    
    // Stack management
    void* stack_memory_;
    size_t current_stack_size_;
    static constexpr size_t INITIAL_STACK_SIZE = 8192;    // 8KB initial
    static constexpr size_t MAX_STACK_SIZE = 1024*1024*1024; // 1GB max
    
    // Async operations
    std::unordered_map<int64_t, std::shared_ptr<AsyncOperation>> pending_async_ops_;
    std::vector<int> active_timer_fds_;
    
    // Promise coordination
    std::unordered_map<int64_t, std::shared_ptr<PromiseState>> promise_states_;
    
    // Parent-child relationships
    std::weak_ptr<Goroutine> parent_;
    std::vector<std::shared_ptr<Goroutine>> children_;
    std::atomic<int> child_count_{0};
    
    // Function to execute
    std::function<void()> main_function_;
    
public:
    // Core lifecycle
    void start();
    void suspend();
    void resume();
    void yield();
    
    // Thread affinity for cache locality
    void set_preferred_thread(int thread_id) { preferred_thread_id_.store(thread_id); }
    int get_preferred_thread() const { return preferred_thread_id_.load(); }
    void clear_preferred_thread() { preferred_thread_id_.store(-1); }  // NEW: explicit clear
    
    // FFI Thread Binding (NEW)
    bool is_ffi_bound() const { return is_ffi_bound_.load(); }
    void set_ffi_bound(bool bound) { is_ffi_bound_.store(bound); }
    void set_bound_ffi_thread(FFIThread* thread) { bound_ffi_thread_ = thread; }
    FFIThread* get_bound_ffi_thread() const { return bound_ffi_thread_; }
    
    // Combined thread identification (NEW)
    bool is_thread_bound() const { 
        return is_ffi_bound() || get_preferred_thread() != -1; 
    }
    
    // Thread migration safety (NEW)
    bool can_migrate_to_ffi() const {
        // Can migrate if not already FFI-bound and not in critical execution state
        return !is_ffi_bound() && state_.load() != GoroutineState::RUNNING;
    }
    
    // Stack management
    void grow_stack(size_t new_size);
    bool check_stack_overflow();
    void setup_guard_page();
    
    // Async coordination
    int64_t add_async_operation(std::shared_ptr<AsyncOperation> op);
    void complete_async_operation(int64_t op_id, void* result);
    
    // Timer management
    int64_t add_timer(int64_t delay_ms, void* callback, bool is_interval);
    void cancel_timer(int64_t timer_id);
    
    // Context switching (assembly functions)
    extern "C" void save_context(GoroutineContext* ctx);
    extern "C" void restore_context(GoroutineContext* ctx);
    extern "C" void switch_context(GoroutineContext* from, GoroutineContext* to);
};
```

### 3. Stack Management System

**Stack Allocation Strategy:**
```cpp
class StackManager {
    // Stack pools for different sizes to avoid fragmentation
    std::vector<void*> stack_pool_8k_;
    std::vector<void*> stack_pool_64k_;
    std::vector<void*> stack_pool_512k_;
    std::mutex pool_mutex_;
    
public:
    void* allocate_stack(size_t size);
    void deallocate_stack(void* stack, size_t size);
    void setup_guard_page(void* stack_base, size_t size);
    
    // Stack growth with pointer updating
    void* grow_stack(void* old_stack, size_t old_size, size_t new_size);
    void update_stack_pointers(void* old_base, void* new_base, size_t size);
};
```

**Stack Overflow Handling:**
```cpp
// Signal handler for SIGSEGV
void stack_overflow_handler(int sig, siginfo_t* info, void* context) {
    // Check if fault address is near current goroutine's guard page
    auto current = get_current_goroutine();
    if (is_stack_overflow(current, info->si_addr)) {
        // Grow stack and continue
        current->grow_stack(current->get_stack_size() * 2);
        return; // Resume execution
    }
    
    // Not a stack overflow, re-raise signal
    raise(SIGSEGV);
}
```

### 4. Context Switching Implementation

**Assembly Context Switch (x86_64):**
```asm
.text
.globl switch_goroutine_context

switch_goroutine_context:
    # RDI = current goroutine context
    # RSI = target goroutine context
    
    # Save current context
    movq %rax, 0(%rdi)      # Save RAX
    movq %rbx, 8(%rdi)      # Save RBX
    movq %rcx, 16(%rdi)     # Save RCX
    movq %rdx, 24(%rdi)     # Save RDX
    movq %rsi, 32(%rdi)     # Save RSI (will be overwritten, but save original)
    # Note: RDI already contains current context pointer
    movq %rbp, 48(%rdi)     # Save RBP
    movq %r8,  56(%rdi)     # Save R8
    movq %r9,  64(%rdi)     # Save R9
    movq %r10, 72(%rdi)     # Save R10
    movq %r11, 80(%rdi)     # Save R11
    movq %r12, 88(%rdi)     # Save R12
    movq %r13, 96(%rdi)     # Save R13
    movq %r14, 104(%rdi)    # Save R14
    movq %r15, 112(%rdi)    # Save R15
    
    # Save stack pointer and flags
    movq %rsp, 120(%rdi)    # Save RSP
    pushfq
    popq %rax
    movq %rax, 128(%rdi)    # Save RFLAGS
    
    # Save floating point state
    leaq 144(%rdi), %rax    # Point to FP save area
    fxsave (%rax)           # Save XMM/FPU state
    
    # Save return address as RIP
    movq (%rsp), %rax
    movq %rax, 136(%rdi)    # Save RIP
    
    # Switch to target goroutine
    # Restore floating point state
    leaq 144(%rsi), %rax    # Point to target FP area
    fxrstor (%rax)          # Restore XMM/FPU state
    
    # Restore stack pointer first
    movq 120(%rsi), %rsp    # Restore RSP
    
    # Push target RIP onto new stack for return
    movq 136(%rsi), %rax
    pushq %rax
    
    # Restore flags
    movq 128(%rsi), %rax
    pushq %rax
    popfq
    
    # Restore general purpose registers
    movq 0(%rsi), %rax      # Restore RAX
    movq 8(%rsi), %rbx      # Restore RBX  
    movq 16(%rsi), %rcx     # Restore RCX
    movq 24(%rsi), %rdx     # Restore RDX
    # Skip RSI restoration for now
    movq 40(%rsi), %rdi     # Restore RDI
    movq 48(%rsi), %rbp     # Restore RBP
    movq 56(%rsi), %r8      # Restore R8
    movq 64(%rsi), %r9      # Restore R9
    movq 72(%rsi), %r10     # Restore R10
    movq 80(%rsi), %r11     # Restore R11
    movq 88(%rsi), %r12     # Restore R12
    movq 96(%rsi), %r13     # Restore R13
    movq 104(%rsi), %r14    # Restore R14
    movq 112(%rsi), %r15    # Restore R15
    
    # Finally restore RSI
    movq 32(%rsi), %rsi     # Restore RSI
    
    ret                     # Jump to target RIP
```

### 5. Async Operation Management

**Unified Async Operation System:**
```cpp
enum class AsyncOpType {
    TIMER,
    HTTP_REQUEST,
    FILE_IO,
    PROMISE_ALL,
    CUSTOM
};

struct AsyncOperation {
    int64_t id;
    AsyncOpType type;
    std::atomic<bool> completed{false};
    void* result_data{nullptr};
    
    // For Promise.all coordination
    std::atomic<int> remaining_count{0};
    std::vector<void*> results;
    std::mutex results_mutex;
    
    // Associated goroutine
    std::weak_ptr<Goroutine> waiting_goroutine;
    
    // Completion callback
    std::function<void(void*)> completion_callback;
};

class AsyncManager {
    std::unordered_map<int64_t, std::shared_ptr<AsyncOperation>> active_ops_;
    std::mutex ops_mutex_;
    std::atomic<int64_t> next_op_id_{1};
    
public:
    int64_t create_async_operation(AsyncOpType type, std::shared_ptr<Goroutine> goroutine);
    void complete_async_operation(int64_t op_id, void* result);
    void handle_promise_all_completion(int64_t op_id, int result_index, void* result);
};
```

### 6. High-Performance Event System

**epoll Integration:**
```cpp
class EventSystem {
    struct ThreadEventLoop {
        int epoll_fd;
        std::unordered_map<int, std::shared_ptr<AsyncOperation>> fd_to_op;
        std::vector<int> timer_fds;
    };
    
    std::vector<ThreadEventLoop> thread_loops_;
    
public:
    void initialize(int num_threads);
    
    // Timer management using timerfd
    int64_t create_timer(int64_t delay_ms, bool is_interval, std::shared_ptr<Goroutine> goroutine);
    void cancel_timer(int64_t timer_id);
    
    // I/O event management
    void add_io_operation(int fd, uint32_t events, std::shared_ptr<AsyncOperation> op);
    void remove_io_operation(int fd);
    
    // Event processing
    void process_events(int thread_id, int timeout_ms);
};
```

**Timer Implementation with timerfd:**
```cpp
int64_t EventSystem::create_timer(int64_t delay_ms, bool is_interval, std::shared_ptr<Goroutine> goroutine) {
    int timer_fd = timerfd_create(CLOCK_MONOTONIC, TFD_CLOEXEC);
    if (timer_fd == -1) return -1;
    
    struct itimerspec timer_spec;
    timer_spec.it_value.tv_sec = delay_ms / 1000;
    timer_spec.it_value.tv_nsec = (delay_ms % 1000) * 1000000;
    
    if (is_interval) {
        timer_spec.it_interval = timer_spec.it_value;
    } else {
        timer_spec.it_interval.tv_sec = 0;
        timer_spec.it_interval.tv_nsec = 0;
    }
    
    if (timerfd_settime(timer_fd, 0, &timer_spec, nullptr) == -1) {
        close(timer_fd);
        return -1;
    }
    
    // Add to epoll
    struct epoll_event event;
    event.events = EPOLLIN;
    event.data.fd = timer_fd;
    
    int thread_id = get_current_thread_id();
    epoll_ctl(thread_loops_[thread_id].epoll_fd, EPOLL_CTL_ADD, timer_fd, &event);
    
    // Create async operation for timer
    auto async_op = std::make_shared<AsyncOperation>();
    async_op->type = AsyncOpType::TIMER;
    async_op->waiting_goroutine = goroutine;
    
    thread_loops_[thread_id].fd_to_op[timer_fd] = async_op;
    
    return timer_fd; // Use fd as timer ID
}
```

### 7. Scheduling System

**Event-Driven Global Scheduling:**
```cpp
class EventDrivenScheduler {
    // Global queues - no per-thread queues
    std::queue<std::shared_ptr<Goroutine>> priority_queue_;    // Timer callbacks, high priority
    std::queue<std::shared_ptr<Goroutine>> regular_queue_;     // Regular async work
    std::mutex queue_mutex_;
    
    // Thread pool management
    std::vector<std::unique_ptr<ThreadWorker>> thread_workers_;
    int num_threads_;
    std::atomic<bool> should_shutdown_{false};
    
    // FFI Thread Pool Integration (NEW)
    std::unique_ptr<FFIThreadPool> ffi_thread_pool_;
    std::atomic<int> total_ffi_bound_goroutines_{0};
    
    // Global scheduling lock to prevent race conditions
    std::mutex scheduling_mutex_;
    
public:
    void initialize();
    void shutdown();
    
    // Main scheduling functions - try idle threads first, queue if none available
    void schedule_priority(std::shared_ptr<Goroutine> goroutine);
    void schedule_regular(std::shared_ptr<Goroutine> goroutine);
    
    // FFI Thread Binding (NEW)
    FFIThread* acquire_ffi_thread() { return ffi_thread_pool_->acquire_thread_for_binding(); }
    void release_ffi_thread(FFIThread* thread) { ffi_thread_pool_->release_thread(thread); }
    bool bind_goroutine_to_ffi_thread(std::shared_ptr<Goroutine> goroutine);
    
    // Queue management for running threads
    std::shared_ptr<Goroutine> try_get_queued_work(int preferred_thread_id);
    
    // Affinity Conflict Resolution (NEW)
    void notify_thread_available(int thread_id);
    void clear_affinity_conflicts_for_ffi_binding(int old_thread_id);
    
private:
    bool try_wake_idle_thread(std::shared_ptr<Goroutine> goroutine);
    bool try_wake_idle_thread_for_queued_work();  // NEW: Handles race condition
    void wake_threads_for_queued_work();
};
```

**Event-Driven Scheduling Algorithm:**
```cpp
void EventDrivenScheduler::schedule_priority(std::shared_ptr<Goroutine> goroutine) {
    // CRITICAL: Atomic operation to prevent lost work
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // 1. Try to wake an idle thread immediately
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread is handling the goroutine
    }
    
    // 2. No idle threads available - queue for later
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        priority_queue_.push(goroutine);
    }
    
    // 3. CRITICAL FIX: Try to wake threads again after queuing
    // This handles the race condition where threads went idle between steps 1 and 2
    if (!try_wake_idle_thread_for_queued_work()) {
        // Still no threads available - work is safely queued and will be picked up
        // when threads finish their current work or new threads become available
    }
}

void EventDrivenScheduler::schedule_regular(std::shared_ptr<Goroutine> goroutine) {
    // CRITICAL: Same race condition fix applied to regular scheduling
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // 1. Try to wake an idle thread immediately
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread is handling the goroutine
    }
    
    // 2. No idle threads available - queue for later
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        regular_queue_.push(goroutine);
    }
    
    // 3. CRITICAL FIX: Try to wake threads again after queuing
    // This handles the race condition where threads went idle between steps 1 and 2
    if (!try_wake_idle_thread_for_queued_work()) {
        // Still no threads available - work is safely queued and will be picked up
        // when threads finish their current work or new threads become available
    }
}

bool EventDrivenScheduler::try_wake_idle_thread(std::shared_ptr<Goroutine> goroutine) {
    // Try threads in order, preferring goroutine's preferred thread
    int preferred = goroutine->get_preferred_thread();
    
    // First try preferred thread if it exists
    if (preferred >= 0 && preferred < num_threads_) {
        if (thread_workers_[preferred]->try_assign_work(goroutine)) {
            return true;
        }
    }
    
    // Then try any idle thread
    for (int i = 0; i < num_threads_; ++i) {
        if (i == preferred) continue; // Already tried
        if (thread_workers_[i]->try_assign_work(goroutine)) {
            return true;
        }
    }
    
    return false; // No idle threads found
}

bool EventDrivenScheduler::try_wake_idle_thread_for_queued_work() {
    // This function specifically handles the race condition case
    // It tries to wake an idle thread to process queued work
    
    for (int i = 0; i < num_threads_; ++i) {
        if (thread_workers_[i]->try_assign_queued_work()) {
            return true; // Successfully woke thread to check queue
        }
    }
    
    return false; // No idle threads found
}

std::shared_ptr<Goroutine> EventDrivenScheduler::try_get_queued_work(int preferred_thread_id) {
    std::lock_guard<std::mutex> lock(queue_mutex_);
    
    // Helper to find preferred work in a queue
    auto find_preferred = [preferred_thread_id](auto& queue) -> std::shared_ptr<Goroutine> {
        // Simple approach: scan queue for preferred thread work
        std::queue<std::shared_ptr<Goroutine>> temp_queue;
        std::shared_ptr<Goroutine> found = nullptr;
        
        while (!queue.empty()) {
            auto item = queue.front();
            queue.pop();
            
            if (!found && item->get_preferred_thread() == preferred_thread_id) {
                found = item; // Found preferred work
            } else {
                temp_queue.push(item); // Keep for later
            }
        }
        
        // Put back non-preferred items
        while (!temp_queue.empty()) {
            queue.push(temp_queue.front());
            temp_queue.pop();
        }
        
        return found;
    };
    
    // 1. Look for preferred work in priority queue
    auto work = find_preferred(priority_queue_);
    if (work) {
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 2. Look for preferred work in regular queue  
    work = find_preferred(regular_queue_);
    if (work) {
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 3. No preferred work - take any priority work
    if (!priority_queue_.empty()) {
        work = priority_queue_.front();
        priority_queue_.pop();
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 4. Finally take any regular work
    if (!regular_queue_.empty()) {
        work = regular_queue_.front();
        regular_queue_.pop();
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    return nullptr; // No work available
}
```

### 8. Race Condition Prevention: Lost Wake-up Fix

**The Problem:**
Classic race condition in event-driven scheduling where work can be "lost":

```
Timeline of Lost Wake-up Race:
1. New goroutine needs scheduling  
2. Scheduler: "Any idle threads?" → NO (all busy)
3. [RACE WINDOW] All threads finish work, check empty queues, go idle
4. Scheduler: Add goroutine to queue
5. Result: Goroutine stuck in queue, all threads sleeping forever!
```

**The Solution: Double-Check Pattern**

The fix requires **two complementary mechanisms**:

#### 1. Enhanced Scheduling with Wake-After-Queue

```cpp
void EventDrivenScheduler::schedule_priority(std::shared_ptr<Goroutine> goroutine) {
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // First attempt: Direct thread wake-up
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread has the work
    }
    
    // No idle threads - safely queue work
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        priority_queue_.push(goroutine);
    }
    
    // CRITICAL: Second attempt after queuing
    // Catches threads that went idle during the race window
    try_wake_idle_thread_for_queued_work();
}
```

#### 2. Thread Worker Queue Check Protocol

```cpp
bool ThreadWorker::try_assign_queued_work() {
    // Wake thread with "check queue" signal (no specific work assigned)
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = nullptr; // Signal: check queue for work
        }
        work_signal_.notify_one();
        return true;
    }
    return false;
}

void ThreadWorker::main_loop() {
    while (!should_exit_.load()) {
        wait_for_work();
        
        if (assigned_work_) {
            // Direct work assignment - execute immediately
            execute_goroutine(assigned_work_);
        } else {
            // Queue check request - look for queued work
            auto queued_work = scheduler.try_get_queued_work(thread_id_);
            if (queued_work) {
                execute_goroutine(queued_work);
            }
        }
        
        assigned_work_ = nullptr;
    }
}
```

**Why This Eliminates the Race Condition:**

✅ **Double-check ensures coverage**: First check + queue + second check  
✅ **Work is never lost**: Either caught by first check or safely queued  
✅ **Threads are never permanently idle**: Second check wakes them after queuing  
✅ **Minimal performance cost**: Second check only runs when first check fails  
✅ **No thundering herd**: Only wakes one thread, not all threads

**Comparison to Alternative Solutions:**

| Solution | Race-Free | Performance | Complexity |
|----------|-----------|-------------|------------|
| **Our Double-Check** | ✅ Yes | ✅ High | ✅ Medium |
| Global Lock Everything | ✅ Yes | ❌ Poor | ✅ Low |
| Periodic Queue Polling | ✅ Yes | ❌ Poor | ✅ Low |
| Condition Variable Broadcast | ✅ Yes | ❌ Thundering Herd | ❌ High |

This approach is **identical to Go's runtime scheduler** which uses the same double-check pattern to prevent lost wake-ups in their goroutine system.

### 9. Promise.all Implementation

**Promise Coordination:**
```cpp
class PromiseAllOperation {
    std::atomic<int> remaining_operations_;
    std::vector<void*> results_;
    std::vector<bool> completed_;
    std::mutex results_mutex_;
    std::shared_ptr<Goroutine> waiting_goroutine_;
    
public:
    PromiseAllOperation(int count, std::shared_ptr<Goroutine> goroutine) 
        : remaining_operations_(count), results_(count), completed_(count, false),
          waiting_goroutine_(goroutine) {}
    
    void complete_operation(int index, void* result) {
        {
            std::lock_guard<std::mutex> lock(results_mutex_);
            if (completed_[index]) return; // Already completed
            
            results_[index] = result;
            completed_[index] = true;
        }
        
        int remaining = remaining_operations_.fetch_sub(1) - 1;
        if (remaining == 0) {
            // All operations complete, resume goroutine
            if (auto goroutine = waiting_goroutine_.lock()) {
                // Store results in goroutine context for retrieval
                goroutine->set_promise_all_results(results_);
                
                // Schedule goroutine for execution
                GoroutineScheduler::instance().schedule(goroutine);
            }
        }
    }
};
```

## FFI Thread Binding Implementation

### The Magic of Zero-Overhead FFI

This is the core innovation that enables both massive concurrency AND zero FFI overhead:

#### 1. Lightweight Start - Heavy Migration

**Initial State**: Every goroutine starts on the lightweight event-driven thread pool
```ultraScript
go async function() {
    console.log("I'm lightweight!");           // Runs on event-driven thread pool
    const result = await someAsyncOp();        // I/O, stays lightweight
    
    const tensor = torch.zeros([100, 100]);    // FIRST FFI CALL - triggers migration!
    // ↑ At this point, goroutine gets bound to dedicated OS thread
    
    const output = model.forward(tensor);      // ZERO overhead - same OS thread
    const loss = criterion(output, target);   // ZERO overhead - same OS thread
}
```

#### 2. Transparent FFI Detection

**JIT-Compiled FFI Call Sites**:
```cpp
// The JIT compiler generates this code for every FFI call site
void compiled_ffi_call_site_123(void* ffi_func, void* args) {
    Goroutine* current = get_current_goroutine();
    
    if (LIKELY(current->is_ffi_bound())) {
        // Fast path: Already bound to OS thread, direct call
        ((torch_zeros_func)ffi_func)(args);
        return;
    }
    
    // Slow path: First FFI call, need to bind to OS thread
    migrate_to_ffi_thread(current, ffi_func, args);
}
```

#### 3. Stack Migration Process

**One-Time Migration** when first FFI call is made:
```cpp
void migrate_to_ffi_thread(Goroutine* goroutine, void* ffi_func, void* args) {
    // 1. Acquire dedicated OS thread from pool
    FFIThread* ffi_thread = global_scheduler.acquire_ffi_thread();
    
    // 2. Handle thread affinity conflicts - clear any existing event-loop thread affinity
    int old_preferred_thread = goroutine->get_preferred_thread();
    if (old_preferred_thread != -1) {
        goroutine->set_preferred_thread(-1);  // Clear event-loop thread affinity
        
        // Notify scheduler that this thread ID may become available for other goroutines
        global_scheduler.notify_thread_available(old_preferred_thread);
    }
    
    // 3. Copy goroutine's stack to native OS thread stack
    void* native_stack = ffi_thread->get_native_stack();
    memcpy(native_stack, goroutine->get_stack(), goroutine->get_stack_size());
    
    // 4. Update stack pointers to new location
    adjust_stack_pointers(goroutine, native_stack);
    
    // 5. Bind goroutine permanently to this OS thread (FFI threads have separate ID space)
    goroutine->set_ffi_bound(true);
    goroutine->set_bound_ffi_thread(ffi_thread);
    ffi_thread->bind_goroutine(goroutine);
    
    // 6. Execute first FFI call on native thread
    ((FFIFunction)ffi_func)(args);
    
    // 7. Continue execution on this OS thread
    ffi_thread->continue_execution(goroutine);
}
```

#### 4. Thread Affinity Conflict Resolution

**The Problem**: When a goroutine with thread affinity gets bound to an FFI thread, what happens to other goroutines that preferred the same event-loop thread?

**The Solution**: **Graceful Affinity Migration**
```cpp
void EventDrivenScheduler::clear_affinity_conflicts_for_ffi_binding(int old_thread_id) {
    std::lock_guard<std::mutex> lock(queue_mutex_);
    
    // Strategy: Don't immediately clear all affinities - that would hurt cache locality
    // Instead, use a "soft migration" approach
    
    int alternative_thread_id = find_least_loaded_thread();
    int goroutines_migrated = 0;
    
    // Check priority queue for conflicting affinities
    for (auto& goroutine : priority_queue_) {
        if (goroutine->get_preferred_thread() == old_thread_id) {
            goroutine->set_preferred_thread(alternative_thread_id);
            goroutines_migrated++;
        }
    }
    
    // Check regular queue for conflicting affinities  
    for (auto& goroutine : regular_queue_) {
        if (goroutine->get_preferred_thread() == old_thread_id) {
            goroutine->set_preferred_thread(alternative_thread_id);
            goroutines_migrated++;
        }
    }
    
    // Log for performance monitoring (optional)
    if (goroutines_migrated > 0) {
        std::cout << "Migrated " << goroutines_migrated 
                  << " goroutine affinities from thread " << old_thread_id 
                  << " to thread " << alternative_thread_id << std::endl;
    }
}
```

**Smart Affinity Strategy**:
1. **Lazy migration**: Only migrate affinity when conflicts are detected, not preemptively
2. **Load balancing**: Migrate conflicted goroutines to least-loaded event-loop thread
3. **Batch migration**: Handle multiple conflicts in one pass to reduce lock contention
4. **No cascade effects**: New thread affinity doesn't trigger further conflicts

**Example Scenario**:
```ultraScript
// Thread 0 has high affinity - many goroutines prefer it for cache locality
go async function webHandler1() { await db.query(...); } // prefers thread 0
go async function webHandler2() { await db.query(...); } // prefers thread 0  
go async function webHandler3() { await db.query(...); } // prefers thread 0

// One goroutine starts doing ML work
go async function mlWorkload() {
    const tensor = torch.zeros([1000, 1000]); // FIRST FFI CALL
    // ↑ This goroutine gets bound to FFI thread, loses thread 0 affinity
    // ↑ webHandler1/2/3 get migrated to thread 1 (least loaded)
    
    const result = model.forward(tensor);      // ZERO overhead on FFI thread
}
```

#### 5. Performance Characteristics

**Before Migration** (Lightweight goroutines):
- Memory per goroutine: ~8KB stack + ~1KB metadata = ~9KB
- Context switch: ~20ns (fiber switching)
- Concurrency limit: ~1 million goroutines
- FFI call overhead: 100-500x (requires migration)

**After Migration** (OS thread-bound):
- Memory per goroutine: ~2MB stack + metadata = ~2MB  
- Context switch: N/A (dedicated thread)
- FFI call overhead: **ZERO** (direct native call)
- Ideal for: ML workloads making thousands of FFI calls

**Affinity Impact** (Thread conflicts):
- Affinity migration overhead: ~1-5μs per conflicted goroutine (rare)
- Cache miss impact: Temporary, rebuilds quickly with new thread assignment
- No performance degradation: Conflicted goroutines just get reassigned to different thread
- Self-healing: System optimizes new affinity patterns automatically

#### 5. Hybrid Performance Benefits

**Web Server Example**: 100,000 HTTP requests
- 99,000 requests: Only I/O operations → Stay lightweight (~900MB total)
- 1,000 requests: Make FFI calls → Get bound to OS threads (~2GB additional)
- **Total**: ~3GB RAM for 100k concurrent requests with ML processing

**ML Training Example**: 1,000 concurrent training jobs  
- All get bound to OS threads immediately on first `torch.zeros()` call
- All subsequent LibTorch calls have zero overhead
- **Total**: ~2GB RAM, maximum FFI performance

### This gives UltraScript the best goroutine system of any language:
✅ **More concurrent than Node.js**: 100k+ goroutines vs ~10k event loop tasks  
✅ **Faster FFI than Go**: Zero overhead vs Go's cgo overhead  
✅ **Simpler than Rust**: No async coloring, automatic optimization  
✅ **More flexible than Java**: Real goroutines vs heavyweight threads

## Critical Implementation Challenges

### 1. Stack Growth and Pointer Updates

**The Problem:**
When a goroutine's stack grows, all pointers pointing into the old stack become invalid.

**Solution:**
```cpp
void Goroutine::grow_stack(size_t new_size) {
    void* old_stack = stack_memory_;
    size_t old_size = current_stack_size_;
    
    // Allocate new larger stack
    void* new_stack = StackManager::instance().allocate_stack(new_size);
    
    // Copy old stack contents
    memcpy(new_stack, old_stack, old_size);
    
    // Update all stack-relative pointers
    update_stack_pointers(old_stack, new_stack, old_size);
    
    // Update stack pointer in context
    uintptr_t offset = static_cast<char*>(context_.rsp) - static_cast<char*>(old_stack);
    context_.rsp = static_cast<char*>(new_stack) + offset;
    
    // Update other stack-relative pointers (rbp, etc.)
    if (context_.rbp >= reinterpret_cast<uintptr_t>(old_stack) && 
        context_.rbp < reinterpret_cast<uintptr_t>(old_stack) + old_size) {
        uintptr_t rbp_offset = context_.rbp - reinterpret_cast<uintptr_t>(old_stack);
        context_.rbp = reinterpret_cast<uintptr_t>(new_stack) + rbp_offset;
    }
    
    // Free old stack
    StackManager::instance().deallocate_stack(old_stack, old_size);
    
    stack_memory_ = new_stack;
    current_stack_size_ = new_size;
}

void Goroutine::update_stack_pointers(void* old_base, void* new_base, size_t size) {
    // Scan through stack and update any pointers that point into old stack
    uintptr_t* stack_ptr = static_cast<uintptr_t*>(new_base);
    uintptr_t old_start = reinterpret_cast<uintptr_t>(old_base);
    uintptr_t old_end = old_start + size;
    uintptr_t offset = reinterpret_cast<uintptr_t>(new_base) - old_start;
    
    for (size_t i = 0; i < size / sizeof(uintptr_t); ++i) {
        if (stack_ptr[i] >= old_start && stack_ptr[i] < old_end) {
            stack_ptr[i] += offset;
        }
    }
}
```

### 2. Signal-Safe Context Switching

**The Problem:**
Signals can interrupt context switches, leading to corruption.

**Solution:**
```cpp
extern "C" void signal_safe_context_switch(GoroutineContext* from, GoroutineContext* to) {
    // Block all signals during context switch
    sigset_t old_mask, block_mask;
    sigfillset(&block_mask);
    pthread_sigmask(SIG_SETMASK, &block_mask, &old_mask);
    
    // Perform context switch
    switch_goroutine_context(from, to);
    
    // Restore signal mask
    pthread_sigmask(SIG_SETMASK, &old_mask, nullptr);
}
```

### 3. Exception Handling Across Context Switches

**The Problem:**
C++ exceptions need to properly unwind across custom stacks.

**Solution:**
```cpp
// Custom exception handling for goroutines
class GoroutineException {
    void* stack_base_;
    size_t stack_size_;
    std::exception_ptr original_exception_;
    
public:
    void capture_exception(void* stack_base, size_t stack_size) {
        stack_base_ = stack_base;
        stack_size_ = stack_size;
        original_exception_ = std::current_exception();
    }
    
    void rethrow_in_context() {
        if (original_exception_) {
            std::rethrow_exception(original_exception_);
        }
    }
};
```

### 4. GC Integration with Custom Stacks

**Stack Scanning for Garbage Collection:**
```cpp
class GCStackScanner {
public:
    void scan_goroutine_stack(std::shared_ptr<Goroutine> goroutine, 
                             std::function<void(void*)> mark_callback) {
        void* stack_base = goroutine->get_stack_base();
        size_t stack_size = goroutine->get_stack_size();
        
        // Conservative stack scanning - treat every pointer-aligned value as potential pointer
        uintptr_t* stack_ptr = static_cast<uintptr_t*>(stack_base);
        size_t num_words = stack_size / sizeof(uintptr_t);
        
        for (size_t i = 0; i < num_words; ++i) {
            if (is_valid_heap_pointer(reinterpret_cast<void*>(stack_ptr[i]))) {
                mark_callback(reinterpret_cast<void*>(stack_ptr[i]));
            }
        }
        
        // Also scan CPU registers for pointers
        scan_registers(goroutine->get_context(), mark_callback);
    }
    
private:
    void scan_registers(const GoroutineContext& ctx, std::function<void(void*)> mark_callback) {
        // Scan general purpose registers
        uintptr_t registers[] = {
            ctx.rax, ctx.rbx, ctx.rcx, ctx.rdx, ctx.rsi, ctx.rdi, ctx.rbp,
            ctx.r8, ctx.r9, ctx.r10, ctx.r11, ctx.r12, ctx.r13, ctx.r14, ctx.r15
        };
        
        for (uintptr_t reg : registers) {
            if (is_valid_heap_pointer(reinterpret_cast<void*>(reg))) {
                mark_callback(reinterpret_cast<void*>(reg));
            }
        }
        
        // TODO: Also scan XMM registers for potential pointers
    }
};
```

## Main Thread Loop Implementation

**Event-Driven Thread Worker with Stack-Safe Trampoline:**

```cpp
enum class ContinuationAction {
    DONE,           // Thread should go idle
    RUN_GOROUTINE,  // Execute this goroutine  
    CHECK_QUEUE     // Check queues for more work
};

struct Continuation {
    ContinuationAction action;
    std::shared_ptr<Goroutine> goroutine;
    
    Continuation(ContinuationAction a, std::shared_ptr<Goroutine> g = nullptr) 
        : action(a), goroutine(g) {}
};

void ThreadWorker::main_loop() {
    while (!should_exit_.load()) {
        // 1. Wait for work assignment (blocks until work arrives)
        wait_for_work();
        
        if (should_exit_.load()) break;
        
        // 2. Determine work type and execute using trampoline pattern
        Continuation cont;
        stack_depth_ = 0;
        
        if (assigned_work_) {
            // Direct work assignment
            cont = Continuation(ContinuationAction::RUN_GOROUTINE, assigned_work_);
        } else {
            // Queue check request (race condition fix)
            cont = Continuation(ContinuationAction::CHECK_QUEUE);
        }
        
        while (cont.action != ContinuationAction::DONE) {
            switch (cont.action) {
                case ContinuationAction::RUN_GOROUTINE:
                    cont = execute_goroutine(cont.goroutine);
                    break;
                    
                case ContinuationAction::CHECK_QUEUE:
                    cont = check_and_get_next_work();
                    break;
                    
                case ContinuationAction::DONE:
                    break;
            }
        }
        
        // 3. Clear work assignment and return to idle state
        assigned_work_ = nullptr;
    }
}

bool ThreadWorker::try_assign_work(std::shared_ptr<Goroutine> goroutine) {
    // Atomic check-and-set to claim this thread
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        // Successfully claimed thread
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = goroutine;
        }
        work_signal_.notify_one();
        return true;
    }
    return false; // Thread was busy
}

bool ThreadWorker::try_assign_queued_work() {
    // NEW FUNCTION: Race condition fix - wake thread to check queues
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        // Successfully claimed idle thread
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = nullptr; // Signal: no direct work, check queue
        }
        work_signal_.notify_one();
        return true;
    }
    return false; // Thread was busy
}

void ThreadWorker::wait_for_work() {
    std::unique_lock<std::mutex> lock(work_mutex_);
    
    // Mark as idle and wait for work assignment
    is_idle_.store(true);
    work_signal_.wait(lock, [this] { 
        return assigned_work_ != nullptr || should_exit_.load(); 
    });
    
    // UPDATED: Handle both direct work assignment and queue check requests
    if (assigned_work_ || !should_exit_.load()) {
        is_idle_.store(false);
    }
}

Continuation ThreadWorker::execute_goroutine(std::shared_ptr<Goroutine> goroutine) {
    // Set thread affinity for cache locality
    goroutine->set_preferred_thread(thread_id_);
    
    // Mark goroutine as running
    bool expected = false;
    if (!goroutine->is_running_.compare_exchange_strong(expected, true)) {
        // Race condition - goroutine already running elsewhere
        return {ContinuationAction::CHECK_QUEUE};
    }
    
    // Execute goroutine (context switch happens here)
    bool completed = run_goroutine_until_yield_or_complete(goroutine);
    
    // Mark as not running
    goroutine->is_running_.store(false);
    
    if (completed) {
        // Goroutine finished - check for more work to avoid going idle
        return {ContinuationAction::CHECK_QUEUE};
    } else {
        // Goroutine yielded (async operation) - thread can go idle
        return {ContinuationAction::DONE};
    }
}

Continuation ThreadWorker::check_and_get_next_work() {
    // Increment stack depth to prevent infinite recursion
    stack_depth_++;
    
    if (stack_depth_ >= MAX_STACK_DEPTH) {
        // Stack getting too deep - queue any remaining work and return
        // This prevents stack overflow in pathological cases
        return {ContinuationAction::DONE};
    }
    
    // Try to get queued work, preferring this thread
    auto next_work = EventDrivenScheduler::instance().try_get_queued_work(thread_id_);
    
    if (next_work) {
        return {ContinuationAction::RUN_GOROUTINE, next_work};
    }
    
    // No more work - thread will go idle
    return {ContinuationAction::DONE};
}
```

**Integration with Async Events:**

```cpp
// Called when timers fire or async operations complete
void EventDrivenScheduler::on_async_event_complete(std::shared_ptr<Goroutine> goroutine, bool is_timer) {
    // Check if goroutine is already running
    bool expected = false;
    if (goroutine->is_running_.compare_exchange_strong(expected, true)) {
        // Not running - schedule it immediately
        if (is_timer) {
            schedule_priority(goroutine);  // Timer callbacks are high priority
        } else {
            schedule_regular(goroutine);   // Regular async completion
        }
        
        // Reset running flag - scheduler will set it when thread picks up work
        goroutine->is_running_.store(false);
    } else {
        // Already running on some thread - the running thread will pick up 
        // the async result when it next checks for completion
        // No action needed here
    }
}

// epoll/timer integration
void EventSystem::process_timer_event(int timer_fd) {
    // Find goroutine associated with this timer
    auto goroutine = find_goroutine_for_timer(timer_fd);
    if (goroutine) {
        EventDrivenScheduler::instance().on_async_event_complete(goroutine, true);
    }
}

void EventSystem::process_io_event(int fd, uint32_t events) {
    // Find async operation associated with this fd
    auto async_op = find_async_op_for_fd(fd);
    if (async_op && async_op->waiting_goroutine) {
        auto goroutine = async_op->waiting_goroutine.lock();
        if (goroutine) {
            // Store I/O result in async operation
            async_op->complete_with_result(create_io_result(fd, events));
            
            // Schedule goroutine to resume
            EventDrivenScheduler::instance().on_async_event_complete(goroutine, false);
        }
    }
}
```

## Remaining Unsolved Issues

### 1. **FFI Integration Complexity**
**Problem:** When calling C libraries from goroutines, those libraries expect a normal OS thread stack. We need to temporarily switch back to the OS thread's stack for FFI calls.

**Partial Solution:** 
```cpp
extern "C" void* call_c_function_safely(void* func_ptr, void* args) {
    // Save current goroutine context
    auto current = get_current_goroutine();
    GoroutineContext goroutine_ctx = current->get_context();
    
    // Switch to OS thread stack
    // TODO: Need to implement this safely
    
    // Call C function
    typedef void* (*CFunc)(void*);
    void* result = reinterpret_cast<CFunc>(func_ptr)(args);
    
    // Switch back to goroutine stack
    // TODO: Restore goroutine context
    
    return result;
}
```

### 2. **Debugging Integration**
**Problem:** GDB and other debuggers don't understand our custom stacks, making debugging extremely difficult.

**Potential Solution:** Need to implement custom debugging hooks and stack unwinding information.

### 3. **Memory Ordering Guarantees**
**Problem:** Ensuring proper memory barriers during context switches on weakly-ordered architectures (though x86_64 is strongly ordered).

### 4. **Real-time Scheduling Guarantees**
**Problem:** Providing bounded latency guarantees for time-critical goroutines.

### 5. **Integration with Existing JavaScript/Node.js Async Patterns**
**Problem:** Mapping JavaScript Promise semantics perfectly onto this system while maintaining performance.

### 6. **Stack Size Prediction**
**Problem:** Determining optimal initial stack sizes for different types of goroutines to minimize growth operations.

### 7. **Cross-Platform Assembly**
**Problem:** This design is x86_64-specific. Need ARM64 and other architecture support.

## Implementation Roadmap: FFI Thread Binding System

### Phase 1: Core FFI Thread Pool (Week 1)
1. **FFIThread class**: Basic OS thread wrapper with goroutine binding capability
2. **FFIThreadPool**: Pre-created pool of 1000 OS threads for binding
3. **Basic migration**: Simple stack copy mechanism for goroutine→OS thread migration
4. **Integration hooks**: JIT compiler integration points for FFI call detection

### Phase 2: Goroutine State Management (Week 2)  
1. **Enhanced Goroutine class**: Add FFI binding flags and thread references
2. **Stack migration logic**: Proper stack pointer adjustment during migration
3. **Context preservation**: Ensure lexical scope and closures work across migration
4. **Error handling**: Graceful fallbacks if FFI thread pool is exhausted

### Phase 3: JIT Compiler Integration (Week 3)
1. **FFI call site detection**: Identify all FFI calls during compilation
2. **Optimized code generation**: Generate fast/slow paths for FFI-bound vs lightweight goroutines  
3. **Runtime integration**: Connect compiled code to FFI thread binding system
4. **Performance monitoring**: Add metrics to track binding efficiency

### Phase 4: Advanced Optimization (Week 4)
1. **Predictive binding**: Analyze function patterns to pre-bind likely FFI-heavy goroutines
2. **Memory optimization**: Efficient stack management and cleanup
3. **Thread affinity**: CPU core affinity for FFI threads to improve cache performance
4. **Load balancing**: Distribute FFI-bound goroutines across available cores

### Key Integration Points

**JIT Compiler Changes**:
```cpp
// Add to x86_codegen_v2.cpp in get_runtime_function_address()
else if (function_name == "execute_ffi_call") {
    return (void*)execute_ffi_call;
} else if (function_name == "migrate_to_ffi_thread") {
    return (void*)migrate_to_ffi_thread;
}
```

**Runtime Function Registration**:
```cpp
// New runtime functions needed in runtime.h
void* execute_ffi_call(Goroutine* current_goroutine, void* ffi_function, void* args);
void* migrate_to_ffi_thread(Goroutine* goroutine, void* ffi_func, void* args);
bool is_goroutine_ffi_bound(Goroutine* goroutine);
```

**LibTorch Integration Example**:
```ultraScript
// This will automatically get zero-overhead FFI after first call
go async function trainModel() {
    const device = torch.device("cuda:0");     // FIRST FFI → bind to OS thread
    const tensor = torch.randn([1000, 1000]);  // ZERO overhead
    const model = createModel().to(device);    // ZERO overhead
    
    for (let epoch = 0; epoch < 1000; epoch++) {
        const output = model.forward(tensor);   // ZERO overhead 
        const loss = criterion(output, target); // ZERO overhead
        loss.backward();                        // ZERO overhead
        optimizer.step();                       // ZERO overhead
    }
}
```

## Conclusion

This Go-style FFI Thread Binding System represents a **breakthrough in goroutine architecture** - solving the fundamental tension between massive concurrency and zero-overhead FFI that has limited other languages.

**🚀 Revolutionary Capabilities:**

**Massive Concurrency**: 100,000+ lightweight goroutines for web servers
- Each HTTP request can have its own goroutine without performance penalty
- Total memory: ~900MB for 100k I/O-bound goroutines
- TypeScript/JavaScript developers get the concurrency model they expect

**Zero FFI Overhead**: LibTorch and ML workloads run at native speed  
- First FFI call automatically binds goroutine to dedicated OS thread
- All subsequent FFI calls have zero overhead (direct function calls)
- ML training pipelines run as fast as pure C++ implementations

**Transparent Operation**: Developers don't choose modes, runtime optimizes automatically
- No `go.thread` vs `go` decisions needed
- Runtime intelligently handles goroutine→OS thread migration
- Best performance automatically, no complexity for developers

**✅ Competitive Advantages Over Other Languages:**

**vs Node.js**: 10x more concurrent connections, real parallelism, zero FFI overhead
**vs Go**: Same concurrency model but faster FFI (Go's cgo has overhead)  
**vs Rust**: No async coloring, automatic optimization, simpler mental model
**vs Python**: True parallelism, orders of magnitude faster, modern syntax
**vs Java**: Lightweight goroutines vs heavyweight threads, better memory usage

**🎯 Perfect for UltraScript's Mission:**

This architecture enables UltraScript to be the **first language optimized for both web servers AND ML workloads**:

- **Web developers**: Get Go-level concurrency with JavaScript familiarity
- **ML engineers**: Get zero-overhead LibTorch integration with modern syntax  
- **Full-stack teams**: One language for frontend, backend, and ML pipelines

**Implementation**: Ready to build - comprehensive design with clear roadmap and integration points identified.

1. **True Zero Idle CPU Usage** - Threads sleep when no work, exactly like Go's scheduler
2. **Event-Driven Scheduling** - Work triggers immediate thread wake-up, no polling overhead  
3. **Stack-Safe Trampoline** - Prevents stack overflow from recursive goroutine execution
4. **Intelligent Thread Affinity** - Cache locality through preferred thread assignment
5. **Priority-Aware Scheduling** - Timer callbacks always run before regular async work
6. **Race-Condition Safe** - Atomic operations prevent lost work during scheduling
7. **Immediate Response Latency** - Sub-microsecond wake-up times vs millisecond polling

**Performance Characteristics:**

- **CPU Usage When Idle**: 0% (vs 0.1-1% for polling approaches)
- **Response Latency**: Immediate (vs bounded by poll interval)
- **Scalability**: Handles thousands of idle threads efficiently  
- **Memory Usage**: Minimal thread overhead when idle
- **Power Efficiency**: Excellent for mobile/embedded applications

**Comparison to Go's Runtime:**

| Feature | UltraScript Design | Go Runtime | Traditional Polling |
|---------|-------------------|------------|-------------------|
| Idle CPU Usage | 0% | ~0% | 0.1-1% |
| Wake-up Latency | Immediate | Immediate | 1ms average |
| Implementation Complexity | Medium | High | Low |
| Priority Scheduling | Built-in | Advanced | Basic |
| Stack Safety | Trampoline | Segmented stacks | N/A |

The implementation will still be extremely complex and will require:

1. **Extensive assembly programming** for context switching
2. **Deep understanding of CPU architectures** and calling conventions  
3. **Complex memory management** for stacks and pointer updates
4. **Integration with system calls** (epoll, timerfd, etc.)
5. **Comprehensive testing** across many edge cases

**Estimated Implementation Time:** 6-12 months for core functionality, plus 6-12 months for optimization and production hardening.

**Recommendation:** This event-driven design represents the optimal balance of performance, efficiency, and maintainability. It provides Go-level performance with JavaScript-friendly semantics, making it ideal for UltraScript's high-performance goals while maintaining the familiar async/await programming model.
