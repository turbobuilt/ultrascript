# UltraScript Goroutine System V2: Go-Style Implementation

## Overview

This document outlines the complete design for implementing a Go-style goroutine system with proper stack switching, thread pools, and high-performance async handling. This is a comprehensive rewrite that will provide true lightweight concurrency with minimal overhead.

## Architecture Decision: Go-Style FFI Thread Binding System

Based on analysis of both web server patterns (100k+ concurrent requests) and ML workloads (thousands of FFI calls), we implement a **Go-style automatic FFI thread binding system** that provides both massive concurrency AND zero FFI overhead transparently.

### Core Innovation: Dynamic Thread Binding

**How it Works**: All goroutines start lightweight on event-driven thread pools. When a goroutine makes its first FFI call, it gets **automatically bound** to a dedicated OS thread where all subsequent FFI calls have zero overhead.

### Web Server Goroutines (Stay Lightweight)
```ultraScript
// Handles 100k+ concurrent requests - never makes FFI calls, stays on thread pool
server.on('request', go async (req, res) => {
    const user = await database.getUser(req.userId);  // I/O operation, stays lightweight
    const data = await api.fetchUserData(user.id);    // HTTP call, stays lightweight
    res.json(data);
});
```

### ML/FFI Goroutines (Auto-Bind to Threads)
```ultraScript
// Automatically gets bound to OS thread on first FFI call
go async function trainModel() {
    const tensor = torch.zeros([1000, 1000]); // FIRST FFI CALL -> goroutine bound to OS thread
    const result = model.forward(tensor);     // ZERO overhead - same thread
    const loss = criterion(result, target);  // ZERO overhead - same thread
    return loss.backward();                   // ZERO overhead - same thread
}
```

### Benefits of This Approach
✅ **Unlimited web server concurrency**: 100k+ I/O-bound goroutines stay lightweight  
✅ **Zero FFI overhead**: ML goroutines get dedicated threads automatically  
✅ **Transparent operation**: Runtime handles optimization, developers don't choose modes  
✅ **Go-proven architecture**: Based on Go's successful cgo implementation  
✅ **Hybrid scaling**: 100k I/O goroutines + 1k FFI goroutines simultaneously  
✅ **Best of both worlds**: No trade-offs, optimal for all use cases

## Architecture Components

### 1. Thread Pool Management

**Core Design:**
- Fixed number of OS threads (typically `std::thread::hardware_concurrency()`)
- **Event-driven architecture** - threads sleep when idle, wake only when work arrives
- Global priority queues with intelligent thread affinity for cache locality
- Zero CPU usage when no work available (similar to Go's scheduler)

**Thread Worker Structure:**
```cpp
class ThreadWorker {
    int thread_id_;
    std::atomic<bool> is_idle_{true};
    std::atomic<bool> should_exit_{false};
    
    // Work assignment and wake-up mechanism
    std::shared_ptr<Goroutine> assigned_work_{nullptr};
    std::condition_variable work_signal_;
    std::mutex work_mutex_;
    
    // Stack depth tracking for trampoline
    int stack_depth_{0};
    static constexpr int MAX_STACK_DEPTH = 100;
    
public:
    void main_loop();                    // Event-driven main loop
    bool try_assign_work(std::shared_ptr<Goroutine> goroutine);
    bool try_assign_queued_work();      // NEW: Wake thread to check queues (race condition fix)
    void wait_for_work();               // Blocks until work arrives
    void wake_for_work();               // Wakes thread for assigned work
};
```

### FFI Thread Pool for Zero-Overhead FFI Calls

**FFI Thread Management** - dedicated OS threads for FFI-bound goroutines:
```cpp
class FFIThread {
private:
    std::thread native_thread_;
    std::atomic<bool> is_bound_{false};
    std::shared_ptr<Goroutine> bound_goroutine_{nullptr};
    std::condition_variable work_signal_;
    std::mutex work_mutex_;
    
public:
    void bind_goroutine(std::shared_ptr<Goroutine> goroutine);
    void execute_with_native_stack();  // Runs goroutine on native OS thread stack
    bool is_available() const { return !is_bound_; }
    void release_binding();
};

class FFIThreadPool {
private:
    std::vector<std::unique_ptr<FFIThread>> ffi_threads_;
    std::atomic<int> available_count_{1000};  // Pre-create 1000 FFI threads
    std::mutex allocation_mutex_;
    
public:
    FFIThread* acquire_thread_for_binding();
    void release_thread(FFIThread* thread);
    void initialize_pool(int thread_count = 1000);
};
```

### Automatic FFI Detection and Binding

**JIT-Compiled FFI Binding Logic:**
```cpp
// Generated by JIT compiler for FFI calls
void execute_ffi_call(Goroutine* current_goroutine, void* ffi_function, void* args) {
    if (current_goroutine->is_ffi_bound()) {
        // ZERO OVERHEAD PATH: Direct call on dedicated thread
        ((FFIFunction)ffi_function)(args);
    } else {
        // FIRST FFI CALL: Bind goroutine to dedicated OS thread
        FFIThread* ffi_thread = global_scheduler.acquire_ffi_thread();
        ffi_thread->bind_goroutine(current_goroutine);
        current_goroutine->set_ffi_bound(true);
        
        // Migrate stack and execute FFI call on native thread
        ffi_thread->execute_with_native_stack();
        ((FFIFunction)ffi_function)(args);
    }
}
```

### 2. Goroutine Data Structure

**Complete Goroutine State:**
```cpp
struct GoroutineContext {
    // CPU Register State (x86_64)
    uint64_t rax, rbx, rcx, rdx, rsi, rdi, rbp;
    uint64_t r8, r9, r10, r11, r12, r13, r14, r15;
    uint64_t rsp;        // Stack pointer
    uint64_t rip;        // Instruction pointer
    uint64_t rflags;     // CPU flags
    
    // XMM/AVX registers for floating point
    alignas(16) uint8_t xmm_state[512]; // FXSAVE area
    
    // Stack management
    void* stack_base;
    void* stack_top;
    size_t stack_size;
    void* guard_page;
};

class Goroutine {
private:
    int64_t id_;
    std::atomic<GoroutineState> state_{GoroutineState::CREATED};
    std::atomic<bool> is_running_{false};
    std::atomic<int> preferred_thread_id_{-1}; // -1 = no preference, for cache locality
    
    // FFI Thread Binding (NEW)
    std::atomic<bool> is_ffi_bound_{false};     // Has this goroutine been bound to an OS thread?
    FFIThread* bound_ffi_thread_{nullptr};     // Which FFI thread is this bound to?
    std::atomic<int> ffi_call_count_{0};       // Track number of FFI calls for optimization
    
    // Execution context
    GoroutineContext context_;
    
    // Stack management
    void* stack_memory_;
    size_t current_stack_size_;
    static constexpr size_t INITIAL_STACK_SIZE = 8192;    // 8KB initial
    static constexpr size_t MAX_STACK_SIZE = 1024*1024*1024; // 1GB max
    
    // Async operations
    std::unordered_map<int64_t, std::shared_ptr<AsyncOperation>> pending_async_ops_;
    std::vector<int> active_timer_fds_;
    
    // Promise coordination
    std::unordered_map<int64_t, std::shared_ptr<PromiseState>> promise_states_;
    
    // Parent-child relationships
    std::weak_ptr<Goroutine> parent_;
    std::vector<std::shared_ptr<Goroutine>> children_;
    std::atomic<int> child_count_{0};
    
    // Function to execute
    std::function<void()> main_function_;
    
public:
    // Core lifecycle
    void start();
    void suspend();
    void resume();
    void yield();
    
    // Thread affinity for cache locality
    void set_preferred_thread(int thread_id) { preferred_thread_id_.store(thread_id); }
    int get_preferred_thread() const { return preferred_thread_id_.load(); }
    void clear_preferred_thread() { preferred_thread_id_.store(-1); }  // NEW: explicit clear
    
    // FFI Thread Binding (NEW)
    bool is_ffi_bound() const { return is_ffi_bound_.load(); }
    void set_ffi_bound(bool bound) { is_ffi_bound_.store(bound); }
    void set_bound_ffi_thread(FFIThread* thread) { bound_ffi_thread_ = thread; }
    FFIThread* get_bound_ffi_thread() const { return bound_ffi_thread_; }
    
    // Combined thread identification (NEW)
    bool is_thread_bound() const { 
        return is_ffi_bound() || get_preferred_thread() != -1; 
    }
    
    // Thread migration safety (NEW)
    bool can_migrate_to_ffi() const {
        // Can migrate if not already FFI-bound and not in critical execution state
        return !is_ffi_bound() && state_.load() != GoroutineState::RUNNING;
    }
    
    // Stack management
    void grow_stack(size_t new_size);
    bool check_stack_overflow();
    void setup_guard_page();
    
    // Async coordination
    int64_t add_async_operation(std::shared_ptr<AsyncOperation> op);
    void complete_async_operation(int64_t op_id, void* result);
    
    // Timer management
    int64_t add_timer(int64_t delay_ms, void* callback, bool is_interval);
    void cancel_timer(int64_t timer_id);
    
    // Context switching (assembly functions)
    extern "C" void save_context(GoroutineContext* ctx);
    extern "C" void restore_context(GoroutineContext* ctx);
    extern "C" void switch_context(GoroutineContext* from, GoroutineContext* to);
};
```

### 3. Stack Management System

**Stack Allocation Strategy:**
```cpp
class StackManager {
    // Stack pools for different sizes to avoid fragmentation
    std::vector<void*> stack_pool_8k_;
    std::vector<void*> stack_pool_64k_;
    std::vector<void*> stack_pool_512k_;
    std::mutex pool_mutex_;
    
public:
    void* allocate_stack(size_t size);
    void deallocate_stack(void* stack, size_t size);
    void setup_guard_page(void* stack_base, size_t size);
    
    // Stack growth with pointer updating
    void* grow_stack(void* old_stack, size_t old_size, size_t new_size);
    void update_stack_pointers(void* old_base, void* new_base, size_t size);
};
```

**Stack Overflow Handling:**
```cpp
// Signal handler for SIGSEGV
void stack_overflow_handler(int sig, siginfo_t* info, void* context) {
    // Check if fault address is near current goroutine's guard page
    auto current = get_current_goroutine();
    if (is_stack_overflow(current, info->si_addr)) {
        // Grow stack and continue
        current->grow_stack(current->get_stack_size() * 2);
        return; // Resume execution
    }
    
    // Not a stack overflow, re-raise signal
    raise(SIGSEGV);
}
```

### 4. Context Switching Implementation

**Assembly Context Switch (x86_64):**
```asm
.text
.globl switch_goroutine_context

switch_goroutine_context:
    # RDI = current goroutine context
    # RSI = target goroutine context
    
    # Save current context
    movq %rax, 0(%rdi)      # Save RAX
    movq %rbx, 8(%rdi)      # Save RBX
    movq %rcx, 16(%rdi)     # Save RCX
    movq %rdx, 24(%rdi)     # Save RDX
    movq %rsi, 32(%rdi)     # Save RSI (will be overwritten, but save original)
    # Note: RDI already contains current context pointer
    movq %rbp, 48(%rdi)     # Save RBP
    movq %r8,  56(%rdi)     # Save R8
    movq %r9,  64(%rdi)     # Save R9
    movq %r10, 72(%rdi)     # Save R10
    movq %r11, 80(%rdi)     # Save R11
    movq %r12, 88(%rdi)     # Save R12
    movq %r13, 96(%rdi)     # Save R13
    movq %r14, 104(%rdi)    # Save R14
    movq %r15, 112(%rdi)    # Save R15
    
    # Save stack pointer and flags
    movq %rsp, 120(%rdi)    # Save RSP
    pushfq
    popq %rax
    movq %rax, 128(%rdi)    # Save RFLAGS
    
    # Save floating point state
    leaq 144(%rdi), %rax    # Point to FP save area
    fxsave (%rax)           # Save XMM/FPU state
    
    # Save return address as RIP
    movq (%rsp), %rax
    movq %rax, 136(%rdi)    # Save RIP
    
    # Switch to target goroutine
    # Restore floating point state
    leaq 144(%rsi), %rax    # Point to target FP area
    fxrstor (%rax)          # Restore XMM/FPU state
    
    # Restore stack pointer first
    movq 120(%rsi), %rsp    # Restore RSP
    
    # Push target RIP onto new stack for return
    movq 136(%rsi), %rax
    pushq %rax
    
    # Restore flags
    movq 128(%rsi), %rax
    pushq %rax
    popfq
    
    # Restore general purpose registers
    movq 0(%rsi), %rax      # Restore RAX
    movq 8(%rsi), %rbx      # Restore RBX  
    movq 16(%rsi), %rcx     # Restore RCX
    movq 24(%rsi), %rdx     # Restore RDX
    # Skip RSI restoration for now
    movq 40(%rsi), %rdi     # Restore RDI
    movq 48(%rsi), %rbp     # Restore RBP
    movq 56(%rsi), %r8      # Restore R8
    movq 64(%rsi), %r9      # Restore R9
    movq 72(%rsi), %r10     # Restore R10
    movq 80(%rsi), %r11     # Restore R11
    movq 88(%rsi), %r12     # Restore R12
    movq 96(%rsi), %r13     # Restore R13
    movq 104(%rsi), %r14    # Restore R14
    movq 112(%rsi), %r15    # Restore R15
    
    # Finally restore RSI
    movq 32(%rsi), %rsi     # Restore RSI
    
    ret                     # Jump to target RIP
```

### 5. Async Operation Management

**Unified Async Operation System:**
```cpp
enum class AsyncOpType {
    TIMER,
    HTTP_REQUEST,
    FILE_IO,
    PROMISE_ALL,
    CUSTOM
};

struct AsyncOperation {
    int64_t id;
    AsyncOpType type;
    std::atomic<bool> completed{false};
    void* result_data{nullptr};
    
    // For Promise.all coordination
    std::atomic<int> remaining_count{0};
    std::vector<void*> results;
    std::mutex results_mutex;
    
    // Associated goroutine
    std::weak_ptr<Goroutine> waiting_goroutine;
    
    // Completion callback
    std::function<void(void*)> completion_callback;
};

class AsyncManager {
    std::unordered_map<int64_t, std::shared_ptr<AsyncOperation>> active_ops_;
    std::mutex ops_mutex_;
    std::atomic<int64_t> next_op_id_{1};
    
public:
    int64_t create_async_operation(AsyncOpType type, std::shared_ptr<Goroutine> goroutine);
    void complete_async_operation(int64_t op_id, void* result);
    void handle_promise_all_completion(int64_t op_id, int result_index, void* result);
};
```

### 6. High-Performance Event System

**epoll Integration:**
```cpp
class EventSystem {
    struct ThreadEventLoop {
        int epoll_fd;
        std::unordered_map<int, std::shared_ptr<AsyncOperation>> fd_to_op;
        std::vector<int> timer_fds;
    };
    
    std::vector<ThreadEventLoop> thread_loops_;
    
public:
    void initialize(int num_threads);
    
    // Timer management using timerfd
    int64_t create_timer(int64_t delay_ms, bool is_interval, std::shared_ptr<Goroutine> goroutine);
    void cancel_timer(int64_t timer_id);
    
    // I/O event management
    void add_io_operation(int fd, uint32_t events, std::shared_ptr<AsyncOperation> op);
    void remove_io_operation(int fd);
    
    // Event processing
    void process_events(int thread_id, int timeout_ms);
};
```

**Timer Implementation with timerfd:**
```cpp
int64_t EventSystem::create_timer(int64_t delay_ms, bool is_interval, std::shared_ptr<Goroutine> goroutine) {
    int timer_fd = timerfd_create(CLOCK_MONOTONIC, TFD_CLOEXEC);
    if (timer_fd == -1) return -1;
    
    struct itimerspec timer_spec;
    timer_spec.it_value.tv_sec = delay_ms / 1000;
    timer_spec.it_value.tv_nsec = (delay_ms % 1000) * 1000000;
    
    if (is_interval) {
        timer_spec.it_interval = timer_spec.it_value;
    } else {
        timer_spec.it_interval.tv_sec = 0;
        timer_spec.it_interval.tv_nsec = 0;
    }
    
    if (timerfd_settime(timer_fd, 0, &timer_spec, nullptr) == -1) {
        close(timer_fd);
        return -1;
    }
    
    // Add to epoll
    struct epoll_event event;
    event.events = EPOLLIN;
    event.data.fd = timer_fd;
    
    int thread_id = get_current_thread_id();
    epoll_ctl(thread_loops_[thread_id].epoll_fd, EPOLL_CTL_ADD, timer_fd, &event);
    
    // Create async operation for timer
    auto async_op = std::make_shared<AsyncOperation>();
    async_op->type = AsyncOpType::TIMER;
    async_op->waiting_goroutine = goroutine;
    
    thread_loops_[thread_id].fd_to_op[timer_fd] = async_op;
    
    return timer_fd; // Use fd as timer ID
}
```

### 7. Scheduling System

**Event-Driven Global Scheduling:**
```cpp
class EventDrivenScheduler {
    // Global queues - no per-thread queues
    std::queue<std::shared_ptr<Goroutine>> priority_queue_;    // Timer callbacks, high priority
    std::queue<std::shared_ptr<Goroutine>> regular_queue_;     // Regular async work
    std::mutex queue_mutex_;
    
    // Thread pool management
    std::vector<std::unique_ptr<ThreadWorker>> thread_workers_;
    int num_threads_;
    std::atomic<bool> should_shutdown_{false};
    
    // FFI Thread Pool Integration (NEW)
    std::unique_ptr<FFIThreadPool> ffi_thread_pool_;
    std::atomic<int> total_ffi_bound_goroutines_{0};
    
    // Global scheduling lock to prevent race conditions
    std::mutex scheduling_mutex_;
    
public:
    void initialize();
    void shutdown();
    
    // Main scheduling functions - try idle threads first, queue if none available
    void schedule_priority(std::shared_ptr<Goroutine> goroutine);
    void schedule_regular(std::shared_ptr<Goroutine> goroutine);
    
    // FFI Thread Binding (NEW)
    FFIThread* acquire_ffi_thread() { return ffi_thread_pool_->acquire_thread_for_binding(); }
    void release_ffi_thread(FFIThread* thread) { ffi_thread_pool_->release_thread(thread); }
    bool bind_goroutine_to_ffi_thread(std::shared_ptr<Goroutine> goroutine);
    
    // Queue management for running threads
    std::shared_ptr<Goroutine> try_get_queued_work(int preferred_thread_id);
    
    // Affinity Conflict Resolution (NEW)
    void notify_thread_available(int thread_id);
    void clear_affinity_conflicts_for_ffi_binding(int old_thread_id);
    
private:
    bool try_wake_idle_thread(std::shared_ptr<Goroutine> goroutine);
    bool try_wake_idle_thread_for_queued_work();  // NEW: Handles race condition
    void wake_threads_for_queued_work();
};
```

**Event-Driven Scheduling Algorithm:**
```cpp
void EventDrivenScheduler::schedule_priority(std::shared_ptr<Goroutine> goroutine) {
    // CRITICAL: Atomic operation to prevent lost work
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // 1. Try to wake an idle thread immediately
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread is handling the goroutine
    }
    
    // 2. No idle threads available - queue for later
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        priority_queue_.push(goroutine);
    }
    
    // 3. CRITICAL FIX: Try to wake threads again after queuing
    // This handles the race condition where threads went idle between steps 1 and 2
    if (!try_wake_idle_thread_for_queued_work()) {
        // Still no threads available - work is safely queued and will be picked up
        // when threads finish their current work or new threads become available
    }
}

void EventDrivenScheduler::schedule_regular(std::shared_ptr<Goroutine> goroutine) {
    // CRITICAL: Same race condition fix applied to regular scheduling
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // 1. Try to wake an idle thread immediately
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread is handling the goroutine
    }
    
    // 2. No idle threads available - queue for later
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        regular_queue_.push(goroutine);
    }
    
    // 3. CRITICAL FIX: Try to wake threads again after queuing
    // This handles the race condition where threads went idle between steps 1 and 2
    if (!try_wake_idle_thread_for_queued_work()) {
        // Still no threads available - work is safely queued and will be picked up
        // when threads finish their current work or new threads become available
    }
}

bool EventDrivenScheduler::try_wake_idle_thread(std::shared_ptr<Goroutine> goroutine) {
    // Try threads in order, preferring goroutine's preferred thread
    int preferred = goroutine->get_preferred_thread();
    
    // First try preferred thread if it exists
    if (preferred >= 0 && preferred < num_threads_) {
        if (thread_workers_[preferred]->try_assign_work(goroutine)) {
            return true;
        }
    }
    
    // Then try any idle thread
    for (int i = 0; i < num_threads_; ++i) {
        if (i == preferred) continue; // Already tried
        if (thread_workers_[i]->try_assign_work(goroutine)) {
            return true;
        }
    }
    
    return false; // No idle threads found
}

bool EventDrivenScheduler::try_wake_idle_thread_for_queued_work() {
    // This function specifically handles the race condition case
    // It tries to wake an idle thread to process queued work
    
    for (int i = 0; i < num_threads_; ++i) {
        if (thread_workers_[i]->try_assign_queued_work()) {
            return true; // Successfully woke thread to check queue
        }
    }
    
    return false; // No idle threads found
}

std::shared_ptr<Goroutine> EventDrivenScheduler::try_get_queued_work(int preferred_thread_id) {
    std::lock_guard<std::mutex> lock(queue_mutex_);
    
    // Helper to find preferred work in a queue
    auto find_preferred = [preferred_thread_id](auto& queue) -> std::shared_ptr<Goroutine> {
        // Simple approach: scan queue for preferred thread work
        std::queue<std::shared_ptr<Goroutine>> temp_queue;
        std::shared_ptr<Goroutine> found = nullptr;
        
        while (!queue.empty()) {
            auto item = queue.front();
            queue.pop();
            
            if (!found && item->get_preferred_thread() == preferred_thread_id) {
                found = item; // Found preferred work
            } else {
                temp_queue.push(item); // Keep for later
            }
        }
        
        // Put back non-preferred items
        while (!temp_queue.empty()) {
            queue.push(temp_queue.front());
            temp_queue.pop();
        }
        
        return found;
    };
    
    // 1. Look for preferred work in priority queue
    auto work = find_preferred(priority_queue_);
    if (work) {
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 2. Look for preferred work in regular queue  
    work = find_preferred(regular_queue_);
    if (work) {
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 3. No preferred work - take any priority work
    if (!priority_queue_.empty()) {
        work = priority_queue_.front();
        priority_queue_.pop();
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    // 4. Finally take any regular work
    if (!regular_queue_.empty()) {
        work = regular_queue_.front();
        regular_queue_.pop();
        work->set_preferred_thread(preferred_thread_id);
        return work;
    }
    
    return nullptr; // No work available
}
```

### 8. Race Condition Prevention: Lost Wake-up Fix

**The Problem:**
Classic race condition in event-driven scheduling where work can be "lost":

```
Timeline of Lost Wake-up Race:
1. New goroutine needs scheduling  
2. Scheduler: "Any idle threads?" → NO (all busy)
3. [RACE WINDOW] All threads finish work, check empty queues, go idle
4. Scheduler: Add goroutine to queue
5. Result: Goroutine stuck in queue, all threads sleeping forever!
```

**The Solution: Double-Check Pattern**

The fix requires **two complementary mechanisms**:

#### 1. Enhanced Scheduling with Wake-After-Queue

```cpp
void EventDrivenScheduler::schedule_priority(std::shared_ptr<Goroutine> goroutine) {
    std::lock_guard<std::mutex> global_lock(scheduling_mutex_);
    
    // First attempt: Direct thread wake-up
    if (try_wake_idle_thread(goroutine)) {
        return; // Success - thread has the work
    }
    
    // No idle threads - safely queue work
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        priority_queue_.push(goroutine);
    }
    
    // CRITICAL: Second attempt after queuing
    // Catches threads that went idle during the race window
    try_wake_idle_thread_for_queued_work();
}
```

#### 2. Thread Worker Queue Check Protocol

```cpp
bool ThreadWorker::try_assign_queued_work() {
    // Wake thread with "check queue" signal (no specific work assigned)
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = nullptr; // Signal: check queue for work
        }
        work_signal_.notify_one();
        return true;
    }
    return false;
}

void ThreadWorker::main_loop() {
    while (!should_exit_.load()) {
        wait_for_work();
        
        if (assigned_work_) {
            // Direct work assignment - execute immediately
            execute_goroutine(assigned_work_);
        } else {
            // Queue check request - look for queued work
            auto queued_work = scheduler.try_get_queued_work(thread_id_);
            if (queued_work) {
                execute_goroutine(queued_work);
            }
        }
        
        assigned_work_ = nullptr;
    }
}
```

**Why This Eliminates the Race Condition:**

✅ **Double-check ensures coverage**: First check + queue + second check  
✅ **Work is never lost**: Either caught by first check or safely queued  
✅ **Threads are never permanently idle**: Second check wakes them after queuing  
✅ **Minimal performance cost**: Second check only runs when first check fails  
✅ **No thundering herd**: Only wakes one thread, not all threads

**Comparison to Alternative Solutions:**

| Solution | Race-Free | Performance | Complexity |
|----------|-----------|-------------|------------|
| **Our Double-Check** | ✅ Yes | ✅ High | ✅ Medium |
| Global Lock Everything | ✅ Yes | ❌ Poor | ✅ Low |
| Periodic Queue Polling | ✅ Yes | ❌ Poor | ✅ Low |
| Condition Variable Broadcast | ✅ Yes | ❌ Thundering Herd | ❌ High |

This approach is **identical to Go's runtime scheduler** which uses the same double-check pattern to prevent lost wake-ups in their goroutine system.

### 9. Promise.all Implementation

**Promise Coordination:**
```cpp
class PromiseAllOperation {
    std::atomic<int> remaining_operations_;
    std::vector<void*> results_;
    std::vector<bool> completed_;
    std::mutex results_mutex_;
    std::shared_ptr<Goroutine> waiting_goroutine_;
    
public:
    PromiseAllOperation(int count, std::shared_ptr<Goroutine> goroutine) 
        : remaining_operations_(count), results_(count), completed_(count, false),
          waiting_goroutine_(goroutine) {}
    
    void complete_operation(int index, void* result) {
        {
            std::lock_guard<std::mutex> lock(results_mutex_);
            if (completed_[index]) return; // Already completed
            
            results_[index] = result;
            completed_[index] = true;
        }
        
        int remaining = remaining_operations_.fetch_sub(1) - 1;
        if (remaining == 0) {
            // All operations complete, resume goroutine
            if (auto goroutine = waiting_goroutine_.lock()) {
                // Store results in goroutine context for retrieval
                goroutine->set_promise_all_results(results_);
                
                // Schedule goroutine for execution
                GoroutineScheduler::instance().schedule(goroutine);
            }
        }
    }
};
```

## FFI Thread Binding Implementation

### The Magic of Zero-Overhead FFI

This is the core innovation that enables both massive concurrency AND zero FFI overhead:

#### 1. Lightweight Start - Heavy Migration

**Initial State**: Every goroutine starts on the lightweight event-driven thread pool
```ultraScript
go async function() {
    console.log("I'm lightweight!");           // Runs on event-driven thread pool
    const result = await someAsyncOp();        // I/O, stays lightweight
    
    const tensor = torch.zeros([100, 100]);    // FIRST FFI CALL - triggers migration!
    // ↑ At this point, goroutine gets bound to dedicated OS thread
    
    const output = model.forward(tensor);      // ZERO overhead - same OS thread
    const loss = criterion(output, target);   // ZERO overhead - same OS thread
}
```

#### 2. Transparent FFI Detection

**JIT-Compiled FFI Call Sites**:
```cpp
// The JIT compiler generates this code for every FFI call site
void compiled_ffi_call_site_123(void* ffi_func, void* args) {
    Goroutine* current = get_current_goroutine();
    
    if (LIKELY(current->is_ffi_bound())) {
        // Fast path: Already bound to OS thread, direct call
        ((torch_zeros_func)ffi_func)(args);
        return;
    }
    
    // Slow path: First FFI call, need to bind to OS thread
    migrate_to_ffi_thread(current, ffi_func, args);
}
```

#### 3. Stack Migration Process

**One-Time Migration** when first FFI call is made:
```cpp
void migrate_to_ffi_thread(Goroutine* goroutine, void* ffi_func, void* args) {
    // 1. Acquire dedicated OS thread from pool
    FFIThread* ffi_thread = global_scheduler.acquire_ffi_thread();
    
    // 2. Handle thread affinity conflicts - clear any existing event-loop thread affinity
    int old_preferred_thread = goroutine->get_preferred_thread();
    if (old_preferred_thread != -1) {
        goroutine->set_preferred_thread(-1);  // Clear event-loop thread affinity
        
        // Notify scheduler that this thread ID may become available for other goroutines
        global_scheduler.notify_thread_available(old_preferred_thread);
    }
    
    // 3. Copy goroutine's stack to native OS thread stack
    void* native_stack = ffi_thread->get_native_stack();
    memcpy(native_stack, goroutine->get_stack(), goroutine->get_stack_size());
    
    // 4. Update stack pointers to new location
    adjust_stack_pointers(goroutine, native_stack);
    
    // 5. Bind goroutine permanently to this OS thread (FFI threads have separate ID space)
    goroutine->set_ffi_bound(true);
    goroutine->set_bound_ffi_thread(ffi_thread);
    ffi_thread->bind_goroutine(goroutine);
    
    // 6. Execute first FFI call on native thread
    ((FFIFunction)ffi_func)(args);
    
    // 7. Continue execution on this OS thread
    ffi_thread->continue_execution(goroutine);
}
```

#### 4. Thread Affinity Conflict Resolution

**The Problem**: When a goroutine with thread affinity gets bound to an FFI thread, what happens to other goroutines that preferred the same event-loop thread?

**The Solution**: **Graceful Affinity Migration**
```cpp
void EventDrivenScheduler::clear_affinity_conflicts_for_ffi_binding(int old_thread_id) {
    std::lock_guard<std::mutex> lock(queue_mutex_);
    
    // Strategy: Don't immediately clear all affinities - that would hurt cache locality
    // Instead, use a "soft migration" approach
    
    int alternative_thread_id = find_least_loaded_thread();
    int goroutines_migrated = 0;
    
    // Check priority queue for conflicting affinities
    for (auto& goroutine : priority_queue_) {
        if (goroutine->get_preferred_thread() == old_thread_id) {
            goroutine->set_preferred_thread(alternative_thread_id);
            goroutines_migrated++;
        }
    }
    
    // Check regular queue for conflicting affinities  
    for (auto& goroutine : regular_queue_) {
        if (goroutine->get_preferred_thread() == old_thread_id) {
            goroutine->set_preferred_thread(alternative_thread_id);
            goroutines_migrated++;
        }
    }
    
    // Log for performance monitoring (optional)
    if (goroutines_migrated > 0) {
        std::cout << "Migrated " << goroutines_migrated 
                  << " goroutine affinities from thread " << old_thread_id 
                  << " to thread " << alternative_thread_id << std::endl;
    }
}
```

**Smart Affinity Strategy**:
1. **Lazy migration**: Only migrate affinity when conflicts are detected, not preemptively
2. **Load balancing**: Migrate conflicted goroutines to least-loaded event-loop thread
3. **Batch migration**: Handle multiple conflicts in one pass to reduce lock contention
4. **No cascade effects**: New thread affinity doesn't trigger further conflicts

**Example Scenario**:
```ultraScript
// Thread 0 has high affinity - many goroutines prefer it for cache locality
go async function webHandler1() { await db.query(...); } // prefers thread 0
go async function webHandler2() { await db.query(...); } // prefers thread 0  
go async function webHandler3() { await db.query(...); } // prefers thread 0

// One goroutine starts doing ML work
go async function mlWorkload() {
    const tensor = torch.zeros([1000, 1000]); // FIRST FFI CALL
    // ↑ This goroutine gets bound to FFI thread, loses thread 0 affinity
    // ↑ webHandler1/2/3 get migrated to thread 1 (least loaded)
    
    const result = model.forward(tensor);      // ZERO overhead on FFI thread
}
```

#### 5. Performance Characteristics

**Before Migration** (Lightweight goroutines):
- Memory per goroutine: ~8KB stack + ~1KB metadata = ~9KB
- Context switch: ~20ns (fiber switching)
- Concurrency limit: ~1 million goroutines
- FFI call overhead: 100-500x (requires migration)

**After Migration** (OS thread-bound):
- Memory per goroutine: ~2MB stack + metadata = ~2MB  
- Context switch: N/A (dedicated thread)
- FFI call overhead: **ZERO** (direct native call)
- Ideal for: ML workloads making thousands of FFI calls

**Affinity Impact** (Thread conflicts):
- Affinity migration overhead: ~1-5μs per conflicted goroutine (rare)
- Cache miss impact: Temporary, rebuilds quickly with new thread assignment
- No performance degradation: Conflicted goroutines just get reassigned to different thread
- Self-healing: System optimizes new affinity patterns automatically

#### 5. Hybrid Performance Benefits

**Web Server Example**: 100,000 HTTP requests
- 99,000 requests: Only I/O operations → Stay lightweight (~900MB total)
- 1,000 requests: Make FFI calls → Get bound to OS threads (~2GB additional)
- **Total**: ~3GB RAM for 100k concurrent requests with ML processing

**ML Training Example**: 1,000 concurrent training jobs  
- All get bound to OS threads immediately on first `torch.zeros()` call
- All subsequent LibTorch calls have zero overhead
- **Total**: ~2GB RAM, maximum FFI performance

### This gives UltraScript the best goroutine system of any language:
✅ **More concurrent than Node.js**: 100k+ goroutines vs ~10k event loop tasks  
✅ **Faster FFI than Go**: Zero overhead vs Go's cgo overhead  
✅ **Simpler than Rust**: No async coloring, automatic optimization  
✅ **More flexible than Java**: Real goroutines vs heavyweight threads

## Critical Implementation Challenges

### 1. Stack Growth and Pointer Updates

**The Problem:**
When a goroutine's stack grows, all pointers pointing into the old stack become invalid.

**Solution:**
```cpp
void Goroutine::grow_stack(size_t new_size) {
    void* old_stack = stack_memory_;
    size_t old_size = current_stack_size_;
    
    // Allocate new larger stack
    void* new_stack = StackManager::instance().allocate_stack(new_size);
    
    // Copy old stack contents
    memcpy(new_stack, old_stack, old_size);
    
    // Update all stack-relative pointers
    update_stack_pointers(old_stack, new_stack, old_size);
    
    // Update stack pointer in context
    uintptr_t offset = static_cast<char*>(context_.rsp) - static_cast<char*>(old_stack);
    context_.rsp = static_cast<char*>(new_stack) + offset;
    
    // Update other stack-relative pointers (rbp, etc.)
    if (context_.rbp >= reinterpret_cast<uintptr_t>(old_stack) && 
        context_.rbp < reinterpret_cast<uintptr_t>(old_stack) + old_size) {
        uintptr_t rbp_offset = context_.rbp - reinterpret_cast<uintptr_t>(old_stack);
        context_.rbp = reinterpret_cast<uintptr_t>(new_stack) + rbp_offset;
    }
    
    // Free old stack
    StackManager::instance().deallocate_stack(old_stack, old_size);
    
    stack_memory_ = new_stack;
    current_stack_size_ = new_size;
}

void Goroutine::update_stack_pointers(void* old_base, void* new_base, size_t size) {
    // Scan through stack and update any pointers that point into old stack
    uintptr_t* stack_ptr = static_cast<uintptr_t*>(new_base);
    uintptr_t old_start = reinterpret_cast<uintptr_t>(old_base);
    uintptr_t old_end = old_start + size;
    uintptr_t offset = reinterpret_cast<uintptr_t>(new_base) - old_start;
    
    for (size_t i = 0; i < size / sizeof(uintptr_t); ++i) {
        if (stack_ptr[i] >= old_start && stack_ptr[i] < old_end) {
            stack_ptr[i] += offset;
        }
    }
}
```

### 2. Signal-Safe Context Switching

**The Problem:**
Signals can interrupt context switches, leading to corruption.

**Solution:**
```cpp
extern "C" void signal_safe_context_switch(GoroutineContext* from, GoroutineContext* to) {
    // Block all signals during context switch
    sigset_t old_mask, block_mask;
    sigfillset(&block_mask);
    pthread_sigmask(SIG_SETMASK, &block_mask, &old_mask);
    
    // Perform context switch
    switch_goroutine_context(from, to);
    
    // Restore signal mask
    pthread_sigmask(SIG_SETMASK, &old_mask, nullptr);
}
```

### 3. Exception Handling Across Context Switches

**The Problem:**
C++ exceptions need to properly unwind across custom stacks.

**Solution:**
```cpp
// Custom exception handling for goroutines
class GoroutineException {
    void* stack_base_;
    size_t stack_size_;
    std::exception_ptr original_exception_;
    
public:
    void capture_exception(void* stack_base, size_t stack_size) {
        stack_base_ = stack_base;
        stack_size_ = stack_size;
        original_exception_ = std::current_exception();
    }
    
    void rethrow_in_context() {
        if (original_exception_) {
            std::rethrow_exception(original_exception_);
        }
    }
};
```

### 4. GC Integration with Custom Stacks

**Stack Scanning for Garbage Collection:**
```cpp
class GCStackScanner {
public:
    void scan_goroutine_stack(std::shared_ptr<Goroutine> goroutine, 
                             std::function<void(void*)> mark_callback) {
        void* stack_base = goroutine->get_stack_base();
        size_t stack_size = goroutine->get_stack_size();
        
        // Conservative stack scanning - treat every pointer-aligned value as potential pointer
        uintptr_t* stack_ptr = static_cast<uintptr_t*>(stack_base);
        size_t num_words = stack_size / sizeof(uintptr_t);
        
        for (size_t i = 0; i < num_words; ++i) {
            if (is_valid_heap_pointer(reinterpret_cast<void*>(stack_ptr[i]))) {
                mark_callback(reinterpret_cast<void*>(stack_ptr[i]));
            }
        }
        
        // Also scan CPU registers for pointers
        scan_registers(goroutine->get_context(), mark_callback);
    }
    
private:
    void scan_registers(const GoroutineContext& ctx, std::function<void(void*)> mark_callback) {
        // Scan general purpose registers
        uintptr_t registers[] = {
            ctx.rax, ctx.rbx, ctx.rcx, ctx.rdx, ctx.rsi, ctx.rdi, ctx.rbp,
            ctx.r8, ctx.r9, ctx.r10, ctx.r11, ctx.r12, ctx.r13, ctx.r14, ctx.r15
        };
        
        for (uintptr_t reg : registers) {
            if (is_valid_heap_pointer(reinterpret_cast<void*>(reg))) {
                mark_callback(reinterpret_cast<void*>(reg));
            }
        }
        
        // TODO: Also scan XMM registers for potential pointers
    }
};
```

## Main Thread Loop Implementation

**Event-Driven Thread Worker with Stack-Safe Trampoline:**

```cpp
enum class ContinuationAction {
    DONE,           // Thread should go idle
    RUN_GOROUTINE,  // Execute this goroutine  
    CHECK_QUEUE     // Check queues for more work
};

struct Continuation {
    ContinuationAction action;
    std::shared_ptr<Goroutine> goroutine;
    
    Continuation(ContinuationAction a, std::shared_ptr<Goroutine> g = nullptr) 
        : action(a), goroutine(g) {}
};

void ThreadWorker::main_loop() {
    while (!should_exit_.load()) {
        // 1. Wait for work assignment (blocks until work arrives)
        wait_for_work();
        
        if (should_exit_.load()) break;
        
        // 2. Determine work type and execute using trampoline pattern
        Continuation cont;
        stack_depth_ = 0;
        
        if (assigned_work_) {
            // Direct work assignment
            cont = Continuation(ContinuationAction::RUN_GOROUTINE, assigned_work_);
        } else {
            // Queue check request (race condition fix)
            cont = Continuation(ContinuationAction::CHECK_QUEUE);
        }
        
        while (cont.action != ContinuationAction::DONE) {
            switch (cont.action) {
                case ContinuationAction::RUN_GOROUTINE:
                    cont = execute_goroutine(cont.goroutine);
                    break;
                    
                case ContinuationAction::CHECK_QUEUE:
                    cont = check_and_get_next_work();
                    break;
                    
                case ContinuationAction::DONE:
                    break;
            }
        }
        
        // 3. Clear work assignment and return to idle state
        assigned_work_ = nullptr;
    }
}

bool ThreadWorker::try_assign_work(std::shared_ptr<Goroutine> goroutine) {
    // Atomic check-and-set to claim this thread
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        // Successfully claimed thread
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = goroutine;
        }
        work_signal_.notify_one();
        return true;
    }
    return false; // Thread was busy
}

bool ThreadWorker::try_assign_queued_work() {
    // NEW FUNCTION: Race condition fix - wake thread to check queues
    bool expected = true;
    if (is_idle_.compare_exchange_strong(expected, false)) {
        // Successfully claimed idle thread
        {
            std::lock_guard<std::mutex> lock(work_mutex_);
            assigned_work_ = nullptr; // Signal: no direct work, check queue
        }
        work_signal_.notify_one();
        return true;
    }
    return false; // Thread was busy
}

void ThreadWorker::wait_for_work() {
    std::unique_lock<std::mutex> lock(work_mutex_);
    
    // Mark as idle and wait for work assignment
    is_idle_.store(true);
    work_signal_.wait(lock, [this] { 
        return assigned_work_ != nullptr || should_exit_.load(); 
    });
    
    // UPDATED: Handle both direct work assignment and queue check requests
    if (assigned_work_ || !should_exit_.load()) {
        is_idle_.store(false);
    }
}

Continuation ThreadWorker::execute_goroutine(std::shared_ptr<Goroutine> goroutine) {
    // Set thread affinity for cache locality
    goroutine->set_preferred_thread(thread_id_);
    
    // Mark goroutine as running
    bool expected = false;
    if (!goroutine->is_running_.compare_exchange_strong(expected, true)) {
        // Race condition - goroutine already running elsewhere
        return {ContinuationAction::CHECK_QUEUE};
    }
    
    // Execute goroutine (context switch happens here)
    bool completed = run_goroutine_until_yield_or_complete(goroutine);
    
    // Mark as not running
    goroutine->is_running_.store(false);
    
    if (completed) {
        // Goroutine finished - check for more work to avoid going idle
        return {ContinuationAction::CHECK_QUEUE};
    } else {
        // Goroutine yielded (async operation) - thread can go idle
        return {ContinuationAction::DONE};
    }
}

Continuation ThreadWorker::check_and_get_next_work() {
    // Increment stack depth to prevent infinite recursion
    stack_depth_++;
    
    if (stack_depth_ >= MAX_STACK_DEPTH) {
        // Stack getting too deep - queue any remaining work and return
        // This prevents stack overflow in pathological cases
        return {ContinuationAction::DONE};
    }
    
    // Try to get queued work, preferring this thread
    auto next_work = EventDrivenScheduler::instance().try_get_queued_work(thread_id_);
    
    if (next_work) {
        return {ContinuationAction::RUN_GOROUTINE, next_work};
    }
    
    // No more work - thread will go idle
    return {ContinuationAction::DONE};
}
```

**Integration with Async Events:**

```cpp
// Called when timers fire or async operations complete
void EventDrivenScheduler::on_async_event_complete(std::shared_ptr<Goroutine> goroutine, bool is_timer) {
    // Check if goroutine is already running
    bool expected = false;
    if (goroutine->is_running_.compare_exchange_strong(expected, true)) {
        // Not running - schedule it immediately
        if (is_timer) {
            schedule_priority(goroutine);  // Timer callbacks are high priority
        } else {
            schedule_regular(goroutine);   // Regular async completion
        }
        
        // Reset running flag - scheduler will set it when thread picks up work
        goroutine->is_running_.store(false);
    } else {
        // Already running on some thread - the running thread will pick up 
        // the async result when it next checks for completion
        // No action needed here
    }
}

// epoll/timer integration
void EventSystem::process_timer_event(int timer_fd) {
    // Find goroutine associated with this timer
    auto goroutine = find_goroutine_for_timer(timer_fd);
    if (goroutine) {
        EventDrivenScheduler::instance().on_async_event_complete(goroutine, true);
    }
}

void EventSystem::process_io_event(int fd, uint32_t events) {
    // Find async operation associated with this fd
    auto async_op = find_async_op_for_fd(fd);
    if (async_op && async_op->waiting_goroutine) {
        auto goroutine = async_op->waiting_goroutine.lock();
        if (goroutine) {
            // Store I/O result in async operation
            async_op->complete_with_result(create_io_result(fd, events));
            
            // Schedule goroutine to resume
            EventDrivenScheduler::instance().on_async_event_complete(goroutine, false);
        }
    }
}
```

## Lexical Scope and Variable Management System

### Problem Statement

JavaScript's lexical scoping must work identically in goroutines as in regular functions. Variables captured by goroutines, callbacks, and closures must remain accessible even after parent functions complete. The challenge is balancing performance with JavaScript semantics.

### Solution: Static Analysis with Optimized Assembly Generation

We use **static analysis** at compile time to determine the optimal variable access strategy and generate highly optimized assembly code that minimizes runtime overhead.

### Architecture Overview

#### 1. **Static Analysis Phase**

During compilation, we analyze each function to determine:
- Which variables belong to which lexical scope level
- Which variables are accessed by goroutines, callbacks, or closures
- Optimal memory layout and access patterns

```cpp
struct LexicalScopeInfo {
    int scope_level;              // 0 = current, 1 = parent, 2 = grandparent, etc.
    std::string variable_name;
    size_t offset_in_scope;       // Byte offset within that scope
    bool escapes_current_function; // Captured by goroutines/callbacks
};

class StaticScopeAnalyzer {
private:
    std::unordered_map<std::string, LexicalScopeInfo> variable_scope_map_;
    std::vector<std::vector<std::string>> scope_stack_; // Track nested scopes
    
public:
    struct ScopeOptimizationPlan {
        std::set<int> required_scope_levels;           // Scope addresses to pass down
        std::vector<std::string> variables_to_cache;   // ALL parent scope variables to cache addresses for
    };
    
    void analyze_function(ASTNode* function_node) {
        // 1. Build scope hierarchy for the function
        build_scope_hierarchy(function_node);
        
        // 2. Classify each variable by scope level
        for (auto& var_decl : find_variable_declarations(function_node)) {
            LexicalScopeInfo info;
            info.scope_level = calculate_scope_level(var_decl);
            info.variable_name = var_decl.name;
            info.offset_in_scope = calculate_offset(var_decl);
            info.escapes_current_function = check_if_escapes(var_decl, function_node);
            
            variable_scope_map_[var_decl.name] = info;
        }
        
        // 3. Generate optimization plan for this function
        generate_optimization_plan();
    }
    
    ScopeOptimizationPlan get_optimization_plan() { return optimization_plan_; }
    
private:
    ScopeOptimizationPlan optimization_plan_;
    
    void generate_optimization_plan() {
        // Add ALL parent scope variables to the caching list
        for (auto& [var_name, info] : variable_scope_map_) {
            if (info.scope_level > 0) {
                optimization_plan_.required_scope_levels.insert(info.scope_level);
                optimization_plan_.variables_to_cache.push_back(var_name);
            }
        }
    }
};
```

#### 2. **Optimized Assembly Generation**

Based on static analysis, we generate different assembly patterns optimized for escaping vs non-escaping functions:

**Non-Escaping Functions (Stack-Only Variables):**
```asm
; Direct stack access - no R15 register needed
; All variables at compile-time known offsets from RBP
mov rax, [rbp - 24]    ; Local variable at stack offset -24 (1 instruction)
mov rbx, [rbp - 32]    ; Another local variable at offset -32 (1 instruction)
; Zero overhead - same as normal C/C++ stack variable access
```

**Escaping Functions - Current Scope Variables (Scope Level 0):**
```asm
; R15 points to current lexical scope (heap-allocated for escaping functions)
; Variable at offset 16 in current scope:
mov rax, [r15 + 16]    ; Direct heap scope access - 1 instruction
```

**Escaping Functions - Parent Scope Variables (Scope Level 1+):**

**Smart Register-Based Scope Optimization**
```asm
; OPTIMAL: Only load scope pointers into registers if THIS function accesses them
; Static analysis determines which scopes are actually used by current function

; Example 1: Function accesses parent and grandparent scopes
; Function prologue - load ONLY the scopes we actually use:
mov r12, [rbp - 8]     ; Load parent scope (we access parent variables)
mov r13, [rbp - 16]    ; Load grandparent scope (we access grandparent variables)  
; Skip R14 - we don't access great-grandparent variables in this function

; Variable access becomes 1 instruction for scopes in registers:
mov rax, [r12 + 24]    ; Access parent_var1 - 1 instruction!
mov rbx, [r13 + 32]    ; Access grandparent_var1 - 1 instruction!
inc QWORD PTR [r12 + 40] ; Increment parent_var2 directly - 1 instruction!

; Example 2: Function only accesses parent scope variables
; Function prologue - minimal register usage:
mov r12, [rbp - 8]     ; Only load parent scope (that's all we need)
; R13, R14 available for other optimizations since we don't need them

; Example 3: Function doesn't access parent scopes but children do
; Function prologue - no scope registers needed for this function:
; R12-R14 completely available for other optimizations
; Parent scope pointers still passed on stack for children: [rbp - 8], [rbp - 16], etc.

; Callee-saved registers survive function calls automatically  
call some_function     ; R12, R13 values preserved across call (if loaded)
mov rax, [r12 + 24]    ; Still works after function call - no reload needed!
```

**Fallback: Stack Caching (When Too Many Scopes)**
```asm
; If more than 4 scope levels needed, fall back to stack caching:
; Use registers for first 4 most frequently accessed scopes
; Cache remaining scope addresses on stack

mov r12, [rbp - 8]     ; Parent scope in register
mov r13, [rbp - 16]    ; Grandparent scope in register
; Cache deeper scopes on stack:
mov rax, [rbp - 24]    ; Load great-grandparent scope
add rax, 48            ; Calculate &deep_var address  
mov [rbp - 32], rax    ; Cache deep variable address on stack
```

**Function Setup for Variable Address Caching:**
```asm
; During function prologue, calculate and cache ALL parent scope variable addresses ON STACK
push rbp
mov rbp, rsp
sub rsp, 64                    ; Stack space for cached addresses + locals

; Cache parent scope variable addresses on stack
mov rax, [rbp - 8]             ; Load parent scope address 
add rax, 24                    ; Calculate &parent_var1
mov [rbp - 24], rax            ; Cache parent_var1 address ON STACK at [rbp - 24]

mov rax, [rbp - 8]             ; Load parent scope address
add rax, 32                    ; Calculate &parent_var2  
mov [rbp - 32], rax            ; Cache parent_var2 address ON STACK at [rbp - 32]

mov rax, [rbp - 16]            ; Load grandparent scope address
add rax, 48                    ; Calculate &grandparent_var1
mov [rbp - 40], rax            ; Cache grandparent_var1 address ON STACK at [rbp - 40]
```

#### 3. **Runtime Lexical Scope Layout**

**Stack Frame Structure for Non-Escaping Functions:**
```cpp
struct NonEscapingStackFrame {
    // Standard stack frame
    void* return_address;
    void* saved_rbp;
    
    // Direct local variables (no scope pointers needed)
    int64_t local_var1;          // At [rbp - 16]
    double local_var2;           // At [rbp - 24] 
    std::string local_var3;      // At [rbp - 56]
    // ... compile-time known offsets from RBP
};
```

**Stack Frame Structure for Escaping Functions:**
```cpp
struct EscapingStackFrame {
    // Standard stack frame
    void* return_address;
    void* saved_rbp;
    
    // Lexical scope data - passed down through call chain
    void* current_scope_ptr;      // Always in R15 register
    void* parent_scope_ptr;       // At [rbp - 8] if needed by children
    void* grandparent_scope_ptr;  // At [rbp - 16] if needed by children
    // ... more parent scopes as needed for child functions
    
    // OPTIMIZATION: Cached variable addresses (for functions that access parent variables)
    void* cached_parent_var1_addr;     // At [rbp - 24] - direct access to frequently used parent variable
    void* cached_parent_var2_addr;     // At [rbp - 32] - another parent variable
    void* cached_grandparent_var1_addr; // At [rbp - 40] - grandparent variable
    // ... more cached addresses based on static analysis
    
    // Remaining stack variables (if any) follow
    int64_t temp_var1;
    double temp_var2;
    // ...
};
```

**Lexical Scope Memory Layout (Heap-Allocated for Escaping Functions Only):**
```cpp
struct LexicalScopeMemory {
    // Metadata (for GC and debugging)
    uint32_t scope_id;
    uint32_t variable_count;
    std::atomic<int32_t> ref_count;  // Reference counting for escaping scopes
    
    // Variable storage (determined by static analysis)
    char variable_data[];  // Variables stored at predetermined offsets
};
```

#### 4. **Code Generation Strategy**

**For Non-Escaping Functions (Stack-Only Variables):**
```cpp
void generate_function_without_escaping_variables(FunctionNode* func) {
    // No scope allocation needed - use direct stack offsets
    // R15 register is free for other uses
    
    // Generate standard stack frame setup
    emit_assembly(R"(
        push rbp
        mov rbp, rsp
        sub rsp, %d    ; Allocate space for local variables
    )", total_locals_size);
    
    // Generate variable access with direct stack offsets
    for (auto& var_access : func->variable_accesses) {
        auto stack_offset = calculate_stack_offset(var_access.name);
        
        // Direct stack access - zero overhead
        emit_assembly("mov rax, [rbp - %d]", stack_offset);
    }
    
    // Standard function epilogue
    emit_assembly(R"(
        mov rsp, rbp
        pop rbp
        ret
    )");
}
```

**For Escaping Functions (Heap-Allocated Scopes):**
```cpp
void generate_function_with_escaping_variables(FunctionNode* func) {
    // Allocate lexical scope on heap with reference counting
    emit_assembly(R"(
        ; Standard prologue
        push rbp
        mov rbp, rsp
        sub rsp, %d                    ; Stack space for scope pointers
        
        ; Allocate scope on heap
        mov rdi, %d                    ; Scope size
        call malloc                    ; Allocate heap memory
        mov r15, rax                   ; R15 = current scope pointer
        
        ; Initialize reference count
        mov DWORD PTR [r15 + 8], 1     ; ref_count = 1
        
        ; Copy parent scope pointers if needed
        mov rax, [rbp - 8]             ; Get parent scope (if exists)
        mov [r15 + 12], rax            ; Store parent scope reference
    )", stack_space_needed, scope_size);
    
    // Generate variable access patterns
    for (auto& var_access : func->variable_accesses) {
        auto info = scope_analyzer_.get_variable_info(var_access.name);
        
        if (info.scope_level == 0) {
            // Current scope: direct R15 access
            emit_assembly("mov rax, [r15 + %d]", info.offset_in_scope);
        } else {
            // Parent scope: +1 instruction overhead
            emit_assembly(R"(
                mov rax, [rbp - %d]        ; Load parent scope address
                mov rax, [rax + %d]        ; Access variable
            )", info.scope_level * 8, info.offset_in_scope);
        }
    }
    
    // Add cleanup code
    emit_assembly(R"(
        ; Decrement reference count at function exit
        mov rdi, r15                   ; Current scope
        call decrement_scope_ref_count ; May free if count reaches 0
        
        ; Standard epilogue
        mov rsp, rbp
        pop rbp
        ret
    )");
}
```

#### 5. **Performance Characteristics**

**Variable Access Overhead:**
- **Non-escaping functions**: 0 overhead (direct stack access like C/C++)
- **Escaping functions - current scope**: 0 overhead (direct R15+offset access)
- **Escaping functions - parent scope**: 0 overhead (cached variable addresses - always used)
- **Multiple parent scope levels**: 0 overhead (all variable addresses cached on stack)

**Memory Layout Efficiency:**
- **Non-escaping functions**: Zero malloc overhead, direct stack allocation
- **Escaping functions**: One malloc per function with escaping variables
- **Scope pointers**: Minimal stack space (8 bytes per scope level needed by children)
- **Variable address caching**: Additional stack space for ALL parent variables accessed (8 bytes per cached variable address)

**Simplified Caching Strategy:**
- **All escaping functions**: Always cache addresses of ALL parent scope variables accessed
- **No usage-based decisions**: Eliminates complexity of choosing between strategies
- **Consistent performance**: Every parent scope variable access is exactly 1 instruction

**Assembly Instruction Patterns:**
```asm
; NON-ESCAPING FUNCTIONS (0 overhead):
mov rax, [rbp - offset]           ; 1 instruction - identical to C/C++

; ESCAPING FUNCTIONS - CURRENT SCOPE (0 overhead):
mov rax, [r15 + offset]           ; 1 instruction

; ESCAPING FUNCTIONS - PARENT SCOPE (Always use variable address caching):
mov rax, [rbp - cached_addr_offset] ; 1 instruction - get cached variable address from stack

; EXAMPLE: Function accessing multiple parent scope variables with stack-cached addresses
; Optimized approach (addresses cached on stack during prologue):

; Reading/Writing directly using stack-cached addresses:
mov rax, [rbp - 24]              ; Get cached address of var1 from stack
mov rdx, [rax]                   ; Read var1 value using cached address
mov rax, [rbp - 32]              ; Get cached address of var2 from stack  
mov [rax], rdx                   ; Write to var2 using cached address
mov rax, [rbp - 40]              ; Get cached address of var3 from stack
inc QWORD PTR [rax]              ; Increment var3 directly using cached address

; COMPARISON TO TRADITIONAL APPROACH:
; Traditional JavaScript engines might use:
mov rax, [rbp + closure_ptr]      ; 1. Load closure object
mov rax, [rax + scope_chain]      ; 2. Load scope chain
mov rax, [rax + parent_link]      ; 3. Traverse to parent scope
mov rax, [rax + var_offset]       ; 4. Access variable (4 instructions total)
```

**Register Usage Optimization:**
- **Non-escaping functions**: R15 register available for other optimizations
- **Escaping functions**: R15 dedicated to current scope pointer
- **All functions**: Only use heap allocation when variables actually escape



So we would have to do lexical scope analysis to figure out what scope addresses have to be passed to each child.  In the fastest case with no outer variale scope access, no lexical scope addresses would be passed down. Unfortunately, if a deeper descendent needs it but an intermediate function doesn't, we would still need to include the address in all intermediate functions so that it can be set for the child functions. We would need to keep track of the needed scopes and variables based on static analysis. Then we would do an optimization pass where we essentially compute that grandchild needs grandparent scope and ensure intermediate scopes know they need to get those scope addresses. Then when generating the code for the child functions we would know the offset for each needed scope from the parent function stack and push it onto the child function stack, and the child function code would be generated to be able to access varibles from each of those lexical scopes.




#### 6. **Integration with Goroutine System**

**Goroutine Creation with Scope Capture:**
```cpp
void create_goroutine_with_scope_capture(GoroutineNode* node) {
    auto captured_vars = analyze_captured_variables(node);
    
    if (any_variables_escape(captured_vars)) {
        // Promote current scope to heap before creating goroutine
        emit_assembly(R"(
            ; Convert stack scope to heap scope
            mov rdi, r15                  ; Current scope (stack)
            call promote_scope_to_heap    ; Returns heap scope in rax
            mov r15, rax                  ; Update current scope pointer
            
            ; Increment reference count for goroutine
            inc DWORD PTR [r15 + 8]       ; ref_count++
        )");
    }
    
    // Create goroutine with scope reference
    emit_goroutine_creation_code(node, "r15");
}
```

#### 7. **Debugging and Development Support**

**Debug Information Generation:**
```cpp
struct ScopeDebugInfo {
    uint32_t function_id;
    uint32_t scope_level;
    std::vector<VariableDebugInfo> variables;
    
    struct VariableDebugInfo {
        std::string name;
        size_t offset;
        std::string type;
        bool is_captured;
    };
};

#ifdef DEBUG_MODE
void generate_debug_info(FunctionNode* func) {
    // Generate debug symbols for scope layout
    // Enable scope inspection in debugger
    // Add runtime scope validation checks
}
#endif
```

#### 8. **Memory Management Strategy**

**Reference Counting for Escaping Scopes:**
```cpp
void generate_scope_reference_counting(bool escapes) {
    if (escapes) {
        emit_assembly(R"(
            ; Atomic increment for thread safety
            lock inc DWORD PTR [r15 + 8]   ; Atomic ref_count increment
            
            ; At scope exit:
            lock dec DWORD PTR [r15 + 8]   ; Atomic ref_count decrement
            jz free_scope                  ; Free if count reaches 0
        )");
    }
}

extern "C" void free_scope_if_zero_refs(LexicalScopeMemory* scope) {
    if (scope->ref_count.fetch_sub(1, std::memory_order_acq_rel) == 1) {
        // Call destructors for complex objects
        call_variable_destructors(scope);
        free(scope);
    }
}
```

### Key Benefits of This Approach

1. **Minimal Runtime Overhead**: Static analysis eliminates runtime scope chain traversal
2. **Predictable Performance**: +0 instructions for current scope, +1 for parent scopes
3. **Memory Efficient**: Only save parent scope addresses that are actually accessed
4. **JavaScript Compatible**: Perfect lexical scope semantics maintained
5. **Debugger Friendly**: Clear memory layout with debug information
6. **Thread Safe**: Atomic reference counting for shared scopes

This static analysis approach provides the optimal balance of JavaScript compatibility, runtime performance, and implementation complexity for UltraScript's high-performance goroutine system.

## Remaining Unsolved Issues

### 1. **FFI Integration Complexity**
**Problem:** When calling C libraries from goroutines, those libraries expect a normal OS thread stack. We need to temporarily switch back to the OS thread's stack for FFI calls.

**Partial Solution:** 
```cpp
extern "C" void* call_c_function_safely(void* func_ptr, void* args) {
    // Save current goroutine context
    auto current = get_current_goroutine();
    GoroutineContext goroutine_ctx = current->get_context();
    
    // Switch to OS thread stack
    // TODO: Need to implement this safely
    
    // Call C function
    typedef void* (*CFunc)(void*);
    void* result = reinterpret_cast<CFunc>(func_ptr)(args);
    
    // Switch back to goroutine stack
    // TODO: Restore goroutine context
    
    return result;
}
```

### 2. **Debugging Integration**
**Problem:** GDB and other debuggers don't understand our custom stacks, making debugging extremely difficult.

**Potential Solution:** Need to implement custom debugging hooks and stack unwinding information.

### 3. **Memory Ordering Guarantees**
**Problem:** Ensuring proper memory barriers during context switches on weakly-ordered architectures (though x86_64 is strongly ordered).

### 4. **Real-time Scheduling Guarantees**
**Problem:** Providing bounded latency guarantees for time-critical goroutines.

### 5. **Integration with Existing JavaScript/Node.js Async Patterns**
**Problem:** Mapping JavaScript Promise semantics perfectly onto this system while maintaining performance.

### 6. **Stack Size Prediction**
**Problem:** Determining optimal initial stack sizes for different types of goroutines to minimize growth operations.

### 7. **Cross-Platform Assembly**
**Problem:** This design is x86_64-specific. Need ARM64 and other architecture support.

## Implementation Roadmap: FFI Thread Binding System

### Phase 1: Core FFI Thread Pool (Week 1)
1. **FFIThread class**: Basic OS thread wrapper with goroutine binding capability
2. **FFIThreadPool**: Pre-created pool of 1000 OS threads for binding
3. **Basic migration**: Simple stack copy mechanism for goroutine→OS thread migration
4. **Integration hooks**: JIT compiler integration points for FFI call detection

### Phase 2: Goroutine State Management (Week 2)  
1. **Enhanced Goroutine class**: Add FFI binding flags and thread references
2. **Stack migration logic**: Proper stack pointer adjustment during migration
3. **Context preservation**: Ensure lexical scope and closures work across migration
4. **Error handling**: Graceful fallbacks if FFI thread pool is exhausted

### Phase 3: JIT Compiler Integration (Week 3)
1. **FFI call site detection**: Identify all FFI calls during compilation
2. **Optimized code generation**: Generate fast/slow paths for FFI-bound vs lightweight goroutines  
3. **Runtime integration**: Connect compiled code to FFI thread binding system
4. **Performance monitoring**: Add metrics to track binding efficiency

### Phase 4: Advanced Optimization (Week 4)
1. **Predictive binding**: Analyze function patterns to pre-bind likely FFI-heavy goroutines
2. **Memory optimization**: Efficient stack management and cleanup
3. **Thread affinity**: CPU core affinity for FFI threads to improve cache performance
4. **Load balancing**: Distribute FFI-bound goroutines across available cores

### Key Integration Points

**JIT Compiler Changes**:
```cpp
// Add to x86_codegen_v2.cpp in get_runtime_function_address()
else if (function_name == "execute_ffi_call") {
    return (void*)execute_ffi_call;
} else if (function_name == "migrate_to_ffi_thread") {
    return (void*)migrate_to_ffi_thread;
}
```

**Runtime Function Registration**:
```cpp
// New runtime functions needed in runtime.h
void* execute_ffi_call(Goroutine* current_goroutine, void* ffi_function, void* args);
void* migrate_to_ffi_thread(Goroutine* goroutine, void* ffi_func, void* args);
bool is_goroutine_ffi_bound(Goroutine* goroutine);
```

**LibTorch Integration Example**:
```ultraScript
// This will automatically get zero-overhead FFI after first call
go async function trainModel() {
    const device = torch.device("cuda:0");     // FIRST FFI → bind to OS thread
    const tensor = torch.randn([1000, 1000]);  // ZERO overhead
    const model = createModel().to(device);    // ZERO overhead
    
    for (let epoch = 0; epoch < 1000; epoch++) {
        const output = model.forward(tensor);   // ZERO overhead 
        const loss = criterion(output, target); // ZERO overhead
        loss.backward();                        // ZERO overhead
        optimizer.step();                       // ZERO overhead
    }
}
```

## Conclusion

This Go-style FFI Thread Binding System represents a **breakthrough in goroutine architecture** - solving the fundamental tension between massive concurrency and zero-overhead FFI that has limited other languages.

**🚀 Revolutionary Capabilities:**

**Massive Concurrency**: 100,000+ lightweight goroutines for web servers
- Each HTTP request can have its own goroutine without performance penalty
- Total memory: ~900MB for 100k I/O-bound goroutines
- TypeScript/JavaScript developers get the concurrency model they expect

**Zero FFI Overhead**: LibTorch and ML workloads run at native speed  
- First FFI call automatically binds goroutine to dedicated OS thread
- All subsequent FFI calls have zero overhead (direct function calls)
- ML training pipelines run as fast as pure C++ implementations

**Transparent Operation**: Developers don't choose modes, runtime optimizes automatically
- No `go.thread` vs `go` decisions needed
- Runtime intelligently handles goroutine→OS thread migration
- Best performance automatically, no complexity for developers

**✅ Competitive Advantages Over Other Languages:**

**vs Node.js**: 10x more concurrent connections, real parallelism, zero FFI overhead
**vs Go**: Same concurrency model but faster FFI (Go's cgo has overhead)  
**vs Rust**: No async coloring, automatic optimization, simpler mental model
**vs Python**: True parallelism, orders of magnitude faster, modern syntax
**vs Java**: Lightweight goroutines vs heavyweight threads, better memory usage

**🎯 Perfect for UltraScript's Mission:**

This architecture enables UltraScript to be the **first language optimized for both web servers AND ML workloads**:

- **Web developers**: Get Go-level concurrency with JavaScript familiarity
- **ML engineers**: Get zero-overhead LibTorch integration with modern syntax  
- **Full-stack teams**: One language for frontend, backend, and ML pipelines

**Implementation**: Ready to build - comprehensive design with clear roadmap and integration points identified.

1. **True Zero Idle CPU Usage** - Threads sleep when no work, exactly like Go's scheduler
2. **Event-Driven Scheduling** - Work triggers immediate thread wake-up, no polling overhead  
3. **Stack-Safe Trampoline** - Prevents stack overflow from recursive goroutine execution
4. **Intelligent Thread Affinity** - Cache locality through preferred thread assignment
5. **Priority-Aware Scheduling** - Timer callbacks always run before regular async work
6. **Race-Condition Safe** - Atomic operations prevent lost work during scheduling
7. **Immediate Response Latency** - Sub-microsecond wake-up times vs millisecond polling

**Performance Characteristics:**

- **CPU Usage When Idle**: 0% (vs 0.1-1% for polling approaches)
- **Response Latency**: Immediate (vs bounded by poll interval)
- **Scalability**: Handles thousands of idle threads efficiently  
- **Memory Usage**: Minimal thread overhead when idle
- **Power Efficiency**: Excellent for mobile/embedded applications

**Comparison to Go's Runtime:**

| Feature | UltraScript Design | Go Runtime | Traditional Polling |
|---------|-------------------|------------|-------------------|
| Idle CPU Usage | 0% | ~0% | 0.1-1% |
| Wake-up Latency | Immediate | Immediate | 1ms average |
| Implementation Complexity | Medium | High | Low |
| Priority Scheduling | Built-in | Advanced | Basic |
| Stack Safety | Trampoline | Segmented stacks | N/A |

The implementation will still be extremely complex and will require:

1. **Extensive assembly programming** for context switching
2. **Deep understanding of CPU architectures** and calling conventions  
3. **Complex memory management** for stacks and pointer updates
4. **Integration with system calls** (epoll, timerfd, etc.)
5. **Comprehensive testing** across many edge cases

**Estimated Implementation Time:** 6-12 months for core functionality, plus 6-12 months for optimization and production hardening.

**Recommendation:** This event-driven design represents the optimal balance of performance, efficiency, and maintainability. It provides Go-level performance with JavaScript-friendly semantics, making it ideal for UltraScript's high-performance goals while maintaining the familiar async/await programming model.

## 9. Lexical Scope and Variable Management System

### Problem Statement

JavaScript's lexical scoping must work identically in goroutines as in regular functions. Variables captured by goroutines, callbacks, and closures must remain accessible even after parent functions complete. The challenge is balancing performance with JavaScript semantics.

### Solution: Escape Analysis with Stack/Heap Lexical Scopes

We use **escape analysis** to determine optimal storage strategy per function:

- **Non-escaping variables**: Lexical scope allocated on **stack** (zero malloc overhead)
- **Any escaping variables**: Lexical scope allocated on **heap** (one malloc, reference counted)

### Architecture Overview

#### 1. **Escape Analysis Phase**

```cpp
struct VariableEscapeInfo {
    std::string name;
    bool escapes_to_goroutine;
    bool escapes_to_callback;
    bool escapes_to_return;        // Returned from function
    size_t offset_in_scope;        // Offset within lexical scope
    size_t size;                   // Variable size
};

class EscapeAnalyzer {
public:
    enum class ScopeLocation { STACK, HEAP };
    
    ScopeLocation analyze_function(ASTNode* function, std::vector<VariableEscapeInfo>& variables) {
        // Analyze each variable to see if it escapes
        for (auto& var_decl : function->variable_declarations) {
            VariableEscapeInfo info;
            info.name = var_decl.name;
            info.escapes_to_goroutine = is_captured_by_goroutine(var_decl.name, function);
            info.escapes_to_callback = is_captured_by_callback(var_decl.name, function);
            info.escapes_to_return = is_returned_from_function(var_decl.name, function);
            
            variables.push_back(info);
        }
        
        // Simple rule: if ANY variable escapes, use heap for entire lexical scope
        for (const auto& var : variables) {
            if (var.escapes_to_goroutine || var.escapes_to_callback || var.escapes_to_return) {
                return ScopeLocation::HEAP;
            }
        }
        
        return ScopeLocation::STACK;  // All variables are local - maximum performance
    }
};
```

#### 2. **Stack-Based Lexical Scopes (Maximum Performance)**

For functions with no escaping variables, the lexical scope is allocated directly on the stack:

```asm
; function pureLocal(a, b) { var x = a + b; var y = x * 2; return y; }

pureLocal:
    push rbp
    mov rbp, rsp
    sub rsp, 32                  ; Stack space for lexical scope
                                ; Layout: [rbp-8]=a, [rbp-16]=b, [rbp-24]=x, [rbp-32]=y
    
    ; Initialize lexical scope with parameters
    mov [rbp-8], rdi            ; a = parameter 1
    mov [rbp-16], rsi           ; b = parameter 2
    
    ; var x = a + b (pure stack arithmetic)
    mov rax, [rbp-8]            ; Load a
    add rax, [rbp-16]           ; Add b
    mov [rbp-24], rax           ; Store x
    
    ; var y = x * 2 (pure stack arithmetic)  
    mov rax, [rbp-24]           ; Load x
    shl rax, 1                  ; Multiply by 2
    mov [rbp-32], rax           ; Store y
    
    ; return y
    mov rax, [rbp-32]           ; Load return value
    leave                       ; Automatic stack cleanup
    ret

; Performance: ZERO malloc overhead, maximum variable access speed
```

#### 3. **Heap-Based Lexical Scopes (JavaScript Semantics)**

For functions with escaping variables, the lexical scope is allocated on the heap with reference counting:

```asm
; function withGoroutine(a, b) { 
;   var x = a + b; 
;   var y = x * 2; 
;   go async function() { console.log(x, y); }; 
;   return y; 
; }

withGoroutine:
    push rbp
    mov rbp, rsp
    sub rsp, 8                   ; Stack space for heap pointer storage
    
    ; Allocate lexical scope on heap (ONE malloc for all variables)
    mov rdi, 40                  ; Size: a(8) + b(8) + x(8) + y(8) + ref_count(8)
    call runtime_malloc
    mov [rbp-8], rax            ; Store scope pointer on stack
    mov r15, rax                ; Load scope pointer into R15 for fast access
    
    ; Initialize lexical scope with parameters
    mov [r15 + 0], rdi          ; a = parameter 1
    mov [r15 + 8], rsi          ; b = parameter 2
    mov qword [r15 + 32], 1     ; ref_count = 1
    
    ; var x = a + b (heap variable arithmetic)
    mov rax, [r15 + 0]          ; Load a
    add rax, [r15 + 8]          ; Add b
    mov [r15 + 16], rax         ; Store x
    
    ; var y = x * 2 (heap variable arithmetic)
    mov rax, [r15 + 16]         ; Load x
    shl rax, 1                  ; Multiply by 2
    mov [r15 + 24], rax         ; Store y
    
    ; Create goroutine with captured lexical scope
    mov rdi, [rbp-8]            ; Pass heap scope pointer
    call create_goroutine_with_scope
    mov r15, [rbp-8]            ; RESTORE R15 after function call
    
    ; return y (fast access via R15)
    mov rax, [r15 + 24]         ; Load y
    
    ; Release our reference to lexical scope
    mov rdi, [rbp-8]            ; Load scope pointer
    call runtime_release_scope  ; Decrement ref_count
    
    leave
    ret

; Performance: One malloc overhead + fast variable access via R15
```

#### 4. **Code Generation Strategy**

```cpp
class LexicalScopeCodeGenerator {
public:
    void generate_function(ASTNode* function) {
        std::vector<VariableEscapeInfo> variables;
        auto scope_location = escape_analyzer_.analyze_function(function, variables);
        
        if (scope_location == ScopeLocation::STACK) {
            generate_stack_scope_function(function, variables);
        } else {
            generate_heap_scope_function(function, variables);
        }
    }
    
private:
    void generate_stack_scope_function(ASTNode* function, 
                                      const std::vector<VariableEscapeInfo>& variables) {
        // Calculate stack layout for all variables
        size_t total_stack_size = calculate_stack_layout(variables);
        
        // Generate function prologue with stack allocation
        emit_stack_prologue(total_stack_size);
        
        // All variable access uses direct stack offsets: [rbp + offset]
        for (auto& stmt : function->statements) {
            generate_stack_variable_operations(stmt, variables);
        }
        
        // Stack cleanup is automatic
        emit_stack_epilogue();
    }
    
    void generate_heap_scope_function(ASTNode* function, 
                                     const std::vector<VariableEscapeInfo>& variables) {
        // Calculate heap layout for all variables + ref_count
        size_t scope_object_size = calculate_heap_layout(variables);
        
        // Generate heap allocation and R15 setup
        emit_heap_allocation(scope_object_size);
        emit_r15_scope_setup();
        
        // Variable access uses R15-based addressing: [r15 + offset]
        for (auto& stmt : function->statements) {
            if (is_function_call(stmt)) {
                generate_function_call(stmt);
                emit_r15_restoration();  // mov r15, [rbp-8]
            } else {
                generate_heap_variable_operations(stmt, variables);
            }
        }
        
        // Reference counting cleanup
        emit_scope_reference_release();
    }
    
    void generate_stack_variable_access(const std::string& var_name, 
                                       const std::vector<VariableEscapeInfo>& variables) {
        auto var_info = find_variable(var_name, variables);
        // Direct stack access: mov rax, [rbp + var_info.stack_offset]
        emit_stack_memory_access(var_info.stack_offset);
    }
    
    void generate_heap_variable_access(const std::string& var_name, 
                                      const std::vector<VariableEscapeInfo>& variables) {
        auto var_info = find_variable(var_name, variables);
        // R15-based heap access: mov rax, [r15 + var_info.heap_offset]
        emit_heap_memory_access_via_r15(var_info.heap_offset);
    }
};
```

### 6. **Lexical Scope Lifetime Management**

Managing lexical scope lifetimes is critical for JavaScript semantics, especially with asynchronous operations, event listeners, and nested callbacks that can outlive their parent functions.

#### **Core Challenge: Event-Driven Lifetimes**

Unlike traditional function scopes, JavaScript lexical scopes must survive beyond function execution when captured by:
- Event listeners (potentially indefinite lifetime)
- setTimeout/setInterval callbacks  
- Goroutines and async operations
- Nested callback chains

#### **Thread-Safe Reference Counting System**

```cpp
struct LexicalScope {
    // Thread-safe reference counting with proper memory ordering
    std::atomic<int32_t> total_ref_count{1};
    
    // Detailed reference tracking for debugging and management
    std::atomic<int32_t> parent_function_refs{1};     // Parent function holding reference
    std::atomic<int32_t> event_listener_refs{0};      // Event listeners
    std::atomic<int32_t> timeout_refs{0};             // setTimeout/setInterval callbacks
    std::atomic<int32_t> goroutine_refs{0};           // Active goroutines  
    std::atomic<int32_t> child_scope_refs{0};         // Child scopes depending on this scope
    
    // Scope hierarchy for nested function support
    LexicalScope* parent_scope{nullptr};
    std::vector<LexicalScope*> child_scopes;
    
    // Exception-safe RAII reference management
    class ScopeRef {
        LexicalScope* scope_;
        std::atomic<int32_t>* specific_counter_;
    public:
        ScopeRef(LexicalScope* scope, std::atomic<int32_t>* counter) 
            : scope_(scope), specific_counter_(counter) {
            scope_->total_ref_count.fetch_add(1, std::memory_order_relaxed);
            specific_counter_->fetch_add(1, std::memory_order_relaxed);
        }
        
        ~ScopeRef() {
            if (scope_) {
                specific_counter_->fetch_sub(1, std::memory_order_relaxed);
                scope_->try_release();
            }
        }
        
        // Move-only semantics for exception safety
        ScopeRef(ScopeRef&& other) noexcept 
            : scope_(other.scope_), specific_counter_(other.specific_counter_) {
            other.scope_ = nullptr;
        }
        
        ScopeRef& operator=(ScopeRef&& other) noexcept {
            if (this != &other) {
                if (scope_) {
                    specific_counter_->fetch_sub(1, std::memory_order_relaxed);
                    scope_->try_release();
                }
                scope_ = other.scope_;
                specific_counter_ = other.specific_counter_;
                other.scope_ = nullptr;
            }
            return *this;
        }
    };
    
    // Factory methods for creating exception-safe references
    ScopeRef create_event_listener_ref() {
        return ScopeRef(this, &event_listener_refs);
    }
    
    ScopeRef create_timeout_ref() {
        return ScopeRef(this, &timeout_refs);
    }
    
    ScopeRef create_goroutine_ref() {
        return ScopeRef(this, &goroutine_refs);
    }
    
    void try_release() {
        // Use acquire-release ordering for proper synchronization
        if (total_ref_count.fetch_sub(1, std::memory_order_acq_rel) == 1) {
            // Last reference - safe to delete
            cleanup_and_delete();
        }
    }
    
private:
    void cleanup_and_delete() {
        // Cleanup child scope dependencies
        for (auto* child : child_scopes) {
            child->parent_scope = nullptr;
            child->try_release();
        }
        
        // Release parent scope dependency
        if (parent_scope) {
            parent_scope->remove_child_scope(this);
            parent_scope->try_release();
        }
        
        delete this;
    }
    
    void remove_child_scope(LexicalScope* child) {
        auto it = std::find(child_scopes.begin(), child_scopes.end(), child);
        if (it != child_scopes.end()) {
            child_scopes.erase(it);
            child_scope_refs.fetch_sub(1, std::memory_order_relaxed);
        }
    }
};
```

#### **Event System Integration**

```cpp
class EventLifetimeManager {
    std::unordered_map<void*, std::vector<LexicalScope::ScopeRef>> object_scope_refs_;
    std::mutex refs_mutex_;  // Protect the map during concurrent access
    
public:
    void register_event_listener(void* object, const std::string& event_name, 
                                LexicalScope* scope) {
        std::lock_guard<std::mutex> lock(refs_mutex_);
        
        // Create exception-safe reference that will be automatically released
        object_scope_refs_[object].emplace_back(scope->create_event_listener_ref());
        
        // Register cleanup callback with native event system
        register_object_destructor(object, [this, object]() {
            this->cleanup_object_references(object);
        });
    }
    
    void cleanup_object_references(void* object) {
        std::lock_guard<std::mutex> lock(refs_mutex_);
        
        // Automatic cleanup - RAII ScopeRef destructors handle reference counting
        object_scope_refs_.erase(object);
    }
    
    void cancel_timeout(int timer_id) {
        // When timers are cancelled, corresponding ScopeRef is destroyed
        // automatically releasing the lexical scope reference
        auto it = active_timeouts_.find(timer_id);
        if (it != active_timeouts_.end()) {
            active_timeouts_.erase(it);  // ScopeRef destructor called automatically
        }
    }
    
private:
    std::unordered_map<int, LexicalScope::ScopeRef> active_timeouts_;
};
```

#### **Scope Hierarchy Management**

```cpp
class ScopeHierarchyManager {
public:
    LexicalScope* create_child_scope(LexicalScope* parent) {
        auto* child = new LexicalScope();
        
        if (parent) {
            // Child scope depends on parent - create bidirectional link
            child->parent_scope = parent;
            parent->child_scopes.push_back(child);
            
            // Parent scope gains a reference from child dependency
            parent->total_ref_count.fetch_add(1, std::memory_order_relaxed);
            parent->child_scope_refs.fetch_add(1, std::memory_order_relaxed);
        }
        
        return child;
    }
    
    void handle_nested_callback(LexicalScope* parent_scope, LexicalScope* child_scope) {
        // Callback that needs both parent and child scopes
        auto parent_ref = parent_scope->create_timeout_ref();
        auto child_ref = child_scope->create_timeout_ref();
        
        // Create timeout that captures both references
        timer_system_.schedule(1000, [parent_ref = std::move(parent_ref), 
                                     child_ref = std::move(child_ref)]() {
            // Use both scopes safely
            // References automatically released when lambda is destroyed
        });
    }
};
```

#### **Exception-Safe Callback Execution**

```cpp
class CallbackExecutor {
public:
    void execute_timeout_callback(std::function<void()> callback, 
                                 LexicalScope::ScopeRef scope_ref) {
        try {
            callback();
            // scope_ref automatically released when function exits (success case)
        } catch (...) {
            // scope_ref automatically released when function exits (exception case)
            // RAII ensures no reference leaks
            throw;
        }
    }
    
    void execute_event_callback(std::function<void()> callback,
                               LexicalScope::ScopeRef scope_ref) {
        // Exception-safe wrapper
        struct CallbackWrapper {
            std::function<void()> cb;
            LexicalScope::ScopeRef ref;
            
            void operator()() {
                try {
                    cb();
                } catch (...) {
                    // ref automatically released on exception
                    throw;
                }
                // ref automatically released on success
            }
        };
        
        CallbackWrapper wrapper{std::move(callback), std::move(scope_ref)};
        wrapper();
    }
};
```

#### **Circular Reference Detection and Weak References**

```cpp
class CircularReferenceManager {
    std::unordered_set<LexicalScope*> current_traversal_;
    
public:
    bool detect_cycle(LexicalScope* scope) {
        if (current_traversal_.count(scope)) {
            return true;  // Cycle detected
        }
        
        current_traversal_.insert(scope);
        
        for (auto* child : scope->child_scopes) {
            if (detect_cycle(child)) {
                return true;
            }
        }
        
        current_traversal_.erase(scope);
        return false;
    }
    
    // Weak references for cache-like scenarios where circular refs are expected
    class WeakScopeRef {
        std::weak_ptr<LexicalScope> weak_scope_;
    public:
        WeakScopeRef(LexicalScope* scope) : weak_scope_(scope->shared_from_this()) {}
        
        LexicalScope* try_lock() {
            return weak_scope_.lock().get();
        }
    };
};
```

#### **Debug and Monitoring System**

```cpp
class ScopeLifetimeDebugger {
    std::unordered_map<LexicalScope*, ScopeDebugInfo> scope_info_;
    std::mutex debug_mutex_;
    
    struct ScopeDebugInfo {
        std::string function_name;
        std::chrono::steady_clock::time_point created_at;
        std::vector<std::string> reference_sources;
    };
    
public:
    void register_scope(LexicalScope* scope, const std::string& function_name) {
        std::lock_guard<std::mutex> lock(debug_mutex_);
        scope_info_[scope] = {
            function_name,
            std::chrono::steady_clock::now(),
            {}
        };
    }
    
    void print_scope_status() {
        std::lock_guard<std::mutex> lock(debug_mutex_);
        
        for (auto& [scope, info] : scope_info_) {
            auto age = std::chrono::steady_clock::now() - info.created_at;
            auto age_ms = std::chrono::duration_cast<std::chrono::milliseconds>(age).count();
            
            std::cout << "Scope " << scope << " (" << info.function_name << "):\n"
                     << "  Age: " << age_ms << "ms\n"
                     << "  Total refs: " << scope->total_ref_count.load() << "\n"
                     << "  Event listeners: " << scope->event_listener_refs.load() << "\n"
                     << "  Timeouts: " << scope->timeout_refs.load() << "\n"
                     << "  Goroutines: " << scope->goroutine_refs.load() << "\n"
                     << "  Child scopes: " << scope->child_scope_refs.load() << "\n";
        }
    }
    
    void check_for_leaks() {
        std::lock_guard<std::mutex> lock(debug_mutex_);
        
        for (auto& [scope, info] : scope_info_) {
            auto age = std::chrono::steady_clock::now() - info.created_at;
            auto age_ms = std::chrono::duration_cast<std::chrono::milliseconds>(age).count();
            
            // Flag scopes that have been alive for a long time with references
            if (age_ms > 60000 && scope->total_ref_count.load() > 0) {
                std::cout << "POTENTIAL LEAK: Scope " << scope 
                         << " (" << info.function_name << ") alive for " 
                         << age_ms << "ms with " << scope->total_ref_count.load() 
                         << " references\n";
            }
        }
    }
};
```

#### **Key Design Principles**

1. **RAII Reference Management**: All scope references use RAII wrappers that automatically handle cleanup, preventing leaks even with exceptions

2. **Thread-Safe Operations**: All reference counting uses proper atomic operations with acquire-release memory ordering

3. **Exception Safety**: Reference counting is exception-safe - references are properly released even when callbacks throw

4. **Hierarchical Dependencies**: Child scopes automatically keep parent scopes alive through reference counting

5. **Event System Integration**: Automatic cleanup when objects are destroyed or event listeners are removed

6. **Debug Support**: Comprehensive monitoring and leak detection for development and debugging

This system ensures that lexical scopes remain available exactly as long as needed while preventing memory leaks and handling all the complex lifetime scenarios that JavaScript's event-driven model requires.
```

### Performance Benefits

#### Stack Lexical Scopes (70-80% of functions)
- ✅ **Zero allocation overhead**: Stack pointer movement only (0 cycles)
- ✅ **Zero cleanup overhead**: Automatic when function returns  
- ✅ **Maximum variable access speed**: Direct `[rbp + offset]` addressing
- ✅ **No register constraints**: All registers available for optimization
- ✅ **Perfect for pure computations, loops, mathematical operations**

#### Heap Lexical Scopes (20-30% of functions)  
- ⚡ **Minimal allocation overhead**: One malloc for entire scope (~50-100 cycles)
- ⚡ **Fast variable access**: `[r15 + offset]` after R15 restoration (1-3 cycles)
- ⚡ **Perfect JavaScript semantics**: Variables outlive function scope
- ⚡ **Automatic lifetime management**: Reference counting prevents memory leaks
- ⚡ **Goroutine-compatible**: Variables accessible across threads

### Architecture Portability

The lexical scope approach is architecture-agnostic:

- **x86_64**: Uses R15 register with restoration after calls
- **ARM64**: Uses X19 register (callee-saved, no restoration needed!)  
- **WebAssembly**: Uses local variables (automatically preserved)
- **Future architectures**: Abstract backend system handles differences

### Code Generation Examples

**Pure Stack Function (90%+ of tight loops):**
```ultraScript
function calculateSum(array) {
    var sum = 0;           // Stack variable
    var temp = 0;          // Stack variable
    
    for (var i = 0; i < array.length; i++) {
        temp = array[i] * 2;       // Pure stack arithmetic
        sum += temp;               // Pure stack arithmetic
    }
    return sum;  // Stack return
}
// Assembly: Direct [rbp+offset] access throughout - maximum performance
```

**Heap Scope Function (perfect JavaScript semantics):**
```ultraScript
function withAsyncOperations(data) {
    var processedCount = 0;        // Heap - shared with goroutines
    var results = [];              // Heap - shared with callbacks
    var localTemp = 0;             // Heap - all variables in same scope
    
    for (var i = 0; i < data.length; i++) {
        go async function() {
            results.push(process(data[i]));  // Access heap variables
            processedCount++;                // Atomic increment
        };
    }
    
    setTimeout(() => {
        console.log("Results:", results);    // Access heap variables
    }, 1000);
    
    return processedCount;
}
// Assembly: [r15+offset] access with one malloc for entire scope
```

This approach provides **maximum performance** for pure functions while enabling **perfect JavaScript semantics** when variables need to escape their lexical scope.

#### 4. **Goroutine Integration**

Goroutines automatically capture scope references with zero additional overhead:

```cpp
class Goroutine {
    std::vector<void*> captured_scopes_;  // Scope object references
    
public:
    template<typename ScopeType>
    void capture_scope(ScopeType* scope) {
        scope->add_ref();  // Increment reference count
        captured_scopes_.push_back(scope);
    }
    
    ~Goroutine() {
        // Automatically release all scope references
        for (auto* scope_ptr : captured_scopes_) {
            static_cast<RefCountedScope*>(scope_ptr)->release();
        }
    }
};
```

#### 5. **Lifetime Management**

Reference counting automatically handles complex parent-child relationships:

```ultraScript
function parent() {
    var sharedData = { count: 0 };  // Allocated in ParentFunctionScope
    
    // Child goroutine increments scope reference count
    go async function() {
        await someAsyncOperation();
        sharedData.count++;  // Scope guaranteed to be alive
    };
    
    // setTimeout also increments scope reference count  
    setTimeout(() => {
        sharedData.count++;  // Scope guaranteed to be alive
    }, 5000);
    
    return "done";  // Parent releases its reference, but children keep scope alive
}
```

**Reference Counting Flow:**
1. `parent()` called → scope created with `ref_count = 1`
2. Goroutine created → `ref_count = 2`
3. `setTimeout` created → `ref_count = 3` 
4. `parent()` returns → `ref_count = 2`
5. Goroutine completes → `ref_count = 1`
6. Timer fires and completes → `ref_count = 0` → scope deallocated

### Code Generation Example

**Original UltraScript:**
```ultraScript
function calculateResults(data, threshold) {
    var processed = 0;
    var results = [];
    var total = data.length;
    
    for (var i = 0; i < total; i++) {
        go async function() {
            if (data[i] > threshold) {
                results.push(data[i] * 2);
                processed++;
            }
        };
    }
    
    return processed;
}
```

**Generated JIT Assembly Implementation:**
```asm
; Auto-generated scope object layout (calculated at compile time)
; CalculateResultsFunctionScope:
; [r15 + 0]  = data parameter (DynamicValue)
; [r15 + 8]  = threshold parameter (DynamicValue)  
; [r15 + 16] = processed variable (atomic int64_t)
; [r15 + 24] = results variable (Array)
; [r15 + 32] = total variable (int64_t)
; [r15 + 40] = i variable (int64_t)
; [r15 + 48] = ref_count (atomic int32_t)

calculateResults:
    ; Function prologue
    push rbp
    mov rbp, rsp
    sub rsp, 16                    ; Stack space for scope pointer storage
    
    ; Allocate lexical scope object on heap
    mov rdi, 52                    ; Size of CalculateResultsFunctionScope
    call runtime_malloc
    mov [rbp-8], rax              ; Store scope address on stack
    mov r15, rax                  ; Load scope pointer into R15
    
    ; Initialize scope object with parameters
    mov [r15 + 0], rdi            ; data parameter
    mov [r15 + 8], rsi            ; threshold parameter
    mov qword [r15 + 16], 0       ; processed = 0
    mov qword [r15 + 24], 0       ; results = Array()
    mov qword [r15 + 48], 1       ; ref_count = 1
    
    ; var total = data.length (involves function call)
    mov rdi, [r15 + 0]            ; Load data parameter
    call get_array_length         ; Function call clobbers R15
    mov r15, [rbp-8]              ; RESTORE R15 after function call
    mov [r15 + 32], rax           ; total = array length
    
    ; for (var i = 0; i < total; i++)
    mov qword [r15 + 40], 0       ; i = 0
    
loop_start:
    mov rax, [r15 + 40]           ; Load i
    cmp rax, [r15 + 32]          ; Compare i with total
    jge loop_end                  ; Exit if i >= total
    
    ; Create goroutine with scope capture
    mov rdi, [rbp-8]              ; Pass scope object address
    mov rsi, [r15 + 40]          ; Pass current value of i
    call create_goroutine_with_captured_scope ; Clobbers R15
    mov r15, [rbp-8]              ; RESTORE R15 after function call
    
    ; i++
    inc qword [r15 + 40]          ; Fast increment of i
    jmp loop_start
    
loop_end:
    ; return processed
    mov rax, [r15 + 16]           ; Load processed value
    
    ; Release scope object (decrement ref_count)
    mov rdi, [rbp-8]              ; Load scope address
    call runtime_release_scope    ; Clobbers R15 (but we're done)
    
    leave                         ; Clean up stack
    ret

; Goroutine function (receives scope object as parameter)
goroutine_function:
    push rbp
    mov rbp, rsp
    mov r15, rdi                  ; R15 = captured scope object
    
    ; Increment ref_count atomically
    lock inc dword [r15 + 48]     ; atomic ref_count++
    
    ; if (data[captured_i] > threshold)
    mov rdi, [r15 + 0]            ; Load data array
    mov rsi, rcx                  ; captured_i (passed as parameter)
    call array_get_element        ; Function call
    mov r15, rdi                  ; RESTORE R15 (rdi still has scope address)
    
    ; Compare with threshold
    mov rbx, [r15 + 8]            ; Load threshold
    ; ... comparison logic ...
    
    ; results.push(data[i] * 2)
    mov rdi, [r15 + 24]           ; Load results array
    call array_push               ; Function call
    mov r15, [rbp-8]              ; RESTORE R15 (if we stored scope on stack)
    
    ; processed++ (atomic)
    lock inc qword [r15 + 16]     ; atomic processed++
    
    ; Release scope reference
    lock dec dword [r15 + 48]     ; atomic ref_count--
    jnz goroutine_end             ; Jump if ref_count > 0
    
    ; ref_count == 0, free the scope object
    mov rdi, r15                  ; Pass scope object address
    call runtime_free_scope
    
goroutine_end:
    leave
    ret
```

### Runtime Support Functions

```cpp
extern "C" {
    void* runtime_malloc(size_t size) {
        // Optimized allocator for scope objects
        return aligned_alloc(8, size);  // 8-byte aligned for performance
    }
    
    void runtime_release_scope(void* scope_obj) {
        std::atomic<int>* ref_count = (std::atomic<int>*)((char*)scope_obj + 48);
        if (ref_count->fetch_sub(1, std::memory_order_acq_rel) == 1) {
            // Last reference - free the scope object
            free(scope_obj);
        }
    }
    
    void create_goroutine_with_captured_scope(void* scope_obj, int64_t captured_i) {
        // Increment reference count before creating goroutine
        std::atomic<int>* ref_count = (std::atomic<int>*)((char*)scope_obj + 48);
        ref_count->fetch_add(1, std::memory_order_relaxed);
        
        // Create goroutine with captured scope and loop variable
        goroutine_scheduler.create([scope_obj, captured_i]() {
            goroutine_function(scope_obj, captured_i);
        });
    }
}
```
```

### Performance Benefits

1. **Zero Goroutine Overhead**: Variable capture requires no additional work
2. **Native Speed Access**: Register-based variable access is identical to stack performance
3. **Perfect JavaScript Semantics**: All closures and lexical scope work exactly as expected
4. **Thread-Safe by Design**: Heap allocation allows cross-thread access without special handling
5. **Automatic Memory Management**: Reference counting prevents leaks and use-after-free

### Thread Safety Notes

For maximum performance, variables are **not automatically thread-safe**. Developers use explicit `atomic` operations when needed:

```ultraScript
function threadSafeExample() {
    var counter = 0;  // Unsafe access
    
    go async function() {
        counter++;           // Unsafe - race condition possible
        atomic counter++;    // Safe - uses atomic increment
        atomic {             // Safe block - all operations atomic
            counter += 5;
            console.log(counter);
        }
    };
}
```

for accessing data cross goroutine, for now we do not have thread safety so to speak for performance. we will handle atomics and locking later.